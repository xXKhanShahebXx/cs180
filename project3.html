<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Project 3 — [Auto]Stitching & Photo Mosaics</title>
  <meta name="theme-color" content="#0f1226"/>
  <link rel="stylesheet" href="assets/style.css?v=2"/>
</head>
<body>
  <!-- Top breadcrumb -->
  <nav class="topbar">
    <div class="container" style="padding:10px 24px">
      <div class="breadcrumb">
        <a href="index.html">← Back to Home</a>
        <span>·</span>
        <span>Project 3 — [Auto]Stitching & Photo Mosaics</span>
      </div>
    </div>
  </nav>

  <main class="container">
    <!-- Intro / Overview -->
    <section class="section" id="overview">
      <h1 class="proj-title">Part 3A — Image Warping & Mosaicing</h1>
      <p>In this part of the project I capture overlapping photographs, estimate projective transforms (homographies) from point correspondences, implement inverse warps with nearest-neighbor and bilinear interpolation, and finally blend the registered images into seamless mosaics. I verify correctness by rectifying planar regions.</p>
      <div class="callout"><strong>Deliverables checklist</strong>
        <ul>
          <li>A.1: Two shooting sets with projective relationships (fixed COP).
          <li>A.2: <span class="kbd">computeH(im1_pts, im2_pts)</span> + correspondence visualizations, linear system, recovered <span class="kbd">H</span>.</li>
          <li>A.3: <span class="kbd">warpImageNearestNeighbor</span> and <span class="kbd">warpImageBilinear</span> (inverse warping) + rectification examples.</li>
          <li>A.4: Three mosaics with source images and weighted/blended results; explain the procedure.</li>
        </ul>
      </div>
    </section>

    <!-- ====================== A.1 ====================== -->
    <section class="section" id="a1">
      <h2>A.1 — Shoot the Pictures</h2>
      <p><strong>Setup.</strong> I fix the center of projection and rotate the camera to capture overlapping views. I avoid strong lens distortion and shoot scenes with plentiful texture/structure. Each sequence has ~40–70% overlap.</p>
      <div class="callout"><strong>Process</strong>
      <ul>
      <li>Fix the <em>center of projection</em> (rotate the camera about a pivot; avoid translation) to ensure a projective relationship.</li>
      <li>Target <strong>40–70% overlap</strong> so enough distinct features are shared across images.</li>
      <li>Shoot quickly and, if possible, lock exposure/focus to minimize lighting drift between frames.</li>
      <li>Prefer textured/static scenes; avoid moving subjects when you can.</li>
      <li>Avoid fisheye/strong barrel distortion; moderate/wide angle is fine.</li>
      <li>Capture <strong>≥ 2 sets</strong>; keep orientation consistent and filenames organized for the report.</li>
      </ul>
      </div>
      
      <h3 style="margin-top:12px">Set 1</h3>
      <div class="gallery">
        <figure class="figure"><img src="p3/i11.jpeg" alt="Set 1 — Image 1"/><figcaption class="figcap">Image 1</figcaption></figure>
        <figure class="figure"><img src="p3/i12.jpg" alt="Set 1 — Image 2"/><figcaption class="figcap">Image 2</figcaption></figure>
      </div>

      <h3>Set 2</h3>
      <div class="gallery">
        <figure class="figure"><img src="p3/i21.jpeg" alt="Set 2 — Image 1"/><figcaption class="figcap">Image 1</figcaption></figure>
        <figure class="figure"><img src="p3/i22.jpg" alt="Set 2 — Image 2"/><figcaption class="figcap">Image 2</figcaption></figure>
      </div>
    </section>

    <!-- ====================== A.2 ====================== -->
    <section class="section" id="a2">
      <h2>A.2 — Recover Homographies</h2>
      <p>I estimate a homography <span class="kbd">H</span> such that <span class="kbd">p₁ ≃ H · p₂</span> using clicked correspondences between the two images. With <span class="kbd">n ≥ 4</span> pairs, I solve an overdetermined linear least-squares system.</p>

      <div class="callout"><strong>Process</strong>
<ol>
<li><strong>Collect correspondences</strong> (n ≥ 8 recommended) with my <span class="kbd">collect_matches_ginput</span> helper; keep the order consistent across the two images.</li>
<li><strong>Normalize points</strong> in each image with Hartley’s method (<span class="kbd">_normalize_points_2d</span>): translate to centroid and scale so mean distance is √2.</li>
<li><strong>Build the linear system</strong> <span class="kbd">A h = 0</span> using <span class="kbd">_build_A_dlt</span> from the normalized pairs.</li>
<li><strong>Solve via SVD</strong>: take the right singular vector corresponding to the smallest singular value (<span class="kbd">h = Vt[-1]</span>) and reshape to <span class="kbd">H<sub>norm</sub></span>.</li>
<li><strong>Denormalize</strong>: <span class="kbd">H = T₂⁻¹ · H<sub>norm</sub> · T₁</span> and scale so <span class="kbd">H[2,2] = 1</span>.</li>
<li><strong>Evaluate</strong> with per-point reprojection errors (<span class="kbd">reprojection_errors</span>) and visualize lines (<span class="kbd">show_correspondences</span>).</li>
</ol>
<div class="figcap">Direction: <span class="kbd">computeH(im1_pts, im2_pts)</span> maps <em>im1 → im2</em>; I keep this ordering consistent for warps and mosaics.</div>
</div>
      
      <div class="gallery">
        <figure class="figure"><img src="p3/i1cor.png" alt="Correspondences overlay — Set 1"/><figcaption class="figcap">Set 1: clicked correspondences (visualization).</figcaption></figure>
        <figure class="figure"><img src="p3/i2cor.png" alt="Correspondences overlay — Set 2"/><figcaption class="figcap">Set 2: clicked correspondences (visualization).</figcaption></figure>
      </div>

<div class="figure" style="margin-top:12px">
<pre class="codeblock"><code>Ah=b (8-unknowns with h33=1). Shapes: (20, 8) (20,)
A[:6] =
[[934.5529,  1084.6354,  1.,  0.,  0.,  0.,  -435766.7774,  -505747.7845]
[0.,  0.,  0.,  934.5529,  1084.6354,  1.,  -1011796.8268,  -1174284.1128]
[1069.3355,  1090.5817,  1.,  0.,  0.,  0.,  -661817.1727,  -674966.5627]
[0.,  0.,  0.,  1069.3355,  1090.5817,  1.,  -1153480.5464,  -1176398.6063]
[1120.87,  1146.0804,  1.,  0.,  0.,  0.,  -762583.9786,  -779735.8806]
[0.,  0.,  0.,  1120.87,  1146.0804,  1.,  -1266833.7327,  -1295327.1297]]
b[:6] = [466.2837,  1082.6533,  618.9051,  1078.6891,  680.3501,  1130.2236]</code></pre>
<div class="figcap">System of equations built from clicked correspondences for Set 1.</div>

<div class="figure" style="margin-top:12px">
<pre class="codeblock"><code>Ah=b (8-unknowns with h33=1). Shapes: (20, 8) (20,)
A[:6] =
[[1106.9953,  1183.7402,  1.,  0.,  0.,  0.,  -542503.8985,  -580114.187]
[0.,  0.,  0.,  1106.9953,  1183.7402,  1.,  -1332336.5864,  -1424703.7813]
[1309.1691,  1266.9883,  1.,  0.,  0.,  0.,  -927021.9077,  -897153.6599]
[0.,  0.,  0.,  1309.1691,  1266.9883,  1.,  -1663891.7259,  -1610281.849]
[1233.8495,  817.0524,  1.,  0.,  0.,  0.,  -797874.2383,  -528350.5656]
[0.,  0.,  0.,  1233.8495,  817.0524,  1.,  -1005674.0738,  -665955.1596]]
b[:6] = [490.0688,  1203.5612,  708.0994,  1270.9525,  646.6544,  815.0703]</code></pre>
<div class="figcap">System of equations built from clicked correspondences for Set 2.</div>
</div>
</div>

      <div class="gallery">
        <figure class="figure">
<pre class="codeblock"><code># Recovered H (Set 1)
H_set1 = [
  [1.886814,  -0.046117,  -1070.715416],
  [0.348557,  1.488321,  -442.478074],
  [0.000459,  -0.000046,  +1.0000],
]
</code></pre>
          <figcaption class="figcap">Recovered homography for Set 1.</figcaption>
        </figure>
        <figure class="figure">
<pre class="codeblock"><code># Recovered H (Set 2)
H_set2 = [
  [1.870145,  -0.077241,  -1264.780423],
  [0.359285,  1.504687, -434.648699],
  [0.000459,  -0.000054,  +1.0000],
]
</code></pre>
          <figcaption class="figcap">Recovered homography for Set 2.</figcaption>
        </figure>
      </div>
    </section>

    <!-- ====================== A.3 ====================== -->
    <section class="section" id="a3">
      <h2>A.3 — Warp the Images (Inverse Mapping)</h2>
      <p>Given <span class="kbd">H</span> that maps source→target (<span class="kbd">p_t ≃ H p_s</span>), I create an output canvas, iterate over target pixels, map each location back with <span class="kbd">H⁻¹</span> into the source, and sample using either nearest-neighbor or bilinear interpolation. I also produce an <em>alpha mask</em> to signal valid samples.</p>

<div class="callout"><strong>Process</strong>
<ol>
<li><strong>Canvas sizing</strong>: transform the four source corners through <span class="kbd">H</span> to get a tight bounding box (<span class="kbd">_output_bounds_from_H</span>), or accept a fixed <span class="kbd">output_shape</span>.</li>
<li><strong>Grid back-projection</strong>: make a meshgrid over destination pixels, convert to homogeneous, and map back with <span class="kbd">H⁻¹</span> to get subpixel source coords.</li>
<li><strong>Interpolation</strong>:
<ul>
<li><em>Nearest</em>: round (fast previews).</li>
<li><em>Bilinear</em>: weighted 4-neighborhood (smoother seams).</li>
</ul>
</li>
<li><strong>Validity mask</strong>: mark pixels whose back-projected coords fall inside the source and are finite; I return this as <em>alpha</em> for blending.</li>
<li><strong>Dtypes</strong>: accumulate in float, then convert back to input dtype (with clipping/rounding for uint8); squeeze grayscale dims.</li>
<li><strong>Rectification check</strong>: click four corners of a planar rectangle and map to a canonical square to verify <span class="kbd">computeH</span> + warpers.</li>
</ol>
</div>


      <h3>Rectification</h3>
<p> I click four corners of a planar rectangle in top left → top right → bottom right → bottom left order (or pass <span class="kbd">src_pts</span>). Unless <span class="kbd">n_w</span>/<span class="kbd">n_h</span> are given, I estimate the target width/height from the clicked edge lengths, define a canonical rectangle [0,0] to [W-1,H-1], solve <span class="kbd">H = computeH(src, dst)</span>, and inverse-warp. This validates both <span class="kbd">computeH</span> and the warpers.</p>
      <div class="gallery">
        <figure class="figure"><img src="p3/checker_marked.png" alt="Rectification — original"/><figcaption class="figcap">Original with clicked corners.</figcaption></figure>
        <figure class="figure"><img src="p3/rectified_checker_nn.png" alt="Rectification — Nearest Neighbor"/><figcaption class="figcap">Rectification with Nearest Neighbor interpolation.</figcaption></figure>
        <figure class="figure"><img src="p3/rectified_checker_bi.png" alt="Rectification — Bilinear"/><figcaption class="figcap">Rectification with Bilinear interpolation.</figcaption></figure>
      </div>

      <div class="gallery">
        <figure class="figure"><img src="p3/table_marked.png" alt="Rectification — original"/><figcaption class="figcap">Original with clicked corners.</figcaption></figure>
        <figure class="figure"><img src="p3/rectified_table_nn.png" alt="Rectification — Nearest Neighbor"/><figcaption class="figcap">Rectification with Nearest Neighbor interpolation.</figcaption></figure>
        <figure class="figure"><img src="p3/rectified_table_bi.png" alt="Rectification — Bilinear"/><figcaption class="figcap">Rectification with Bilinear interpolation.</figcaption></figure>
      </div>

      <div class="answer">
        <div class="label">Trade-offs</div>
        <ul>
          <li><strong>Speed:</strong> Nearest neighbor (NN) does a single lookup per pixel; bilinear mixes 4 neighbors. On the same canvas, NN is typically ~1.3–2× faster.</li>
          <li><strong>Quality:</strong> NN introduces stair-stepping and aliasing at edges; bilinear reduces jaggies and yields smoother seams but slightly softens fine texture.</li>
          <li><strong>Use NN</strong> for quick previews/debugging when you want the fastest warp.</li>
          <li><strong>Use bilinear</strong> for final renderings/mosaics to avoid jagged seams; expect a modest speed cost and slight smoothing.</li>
        </ul>
      </div>
    </section>

    <!-- ====================== A.4 ====================== -->
    <section class="section" id="a4">
      <h2>A.4 — Blend the Images into a Mosaic</h2>
      <p>I register images into a common frame and blend overlapping regions. I do weighted averaging via alpha masks.</p>
      <div class="callout"><strong>Implementation details</strong>
      <ol>
      <li><strong>Global frame</strong>: compute a union bounding box with <span class="kbd">_global_bbox_for_mosaic</span> using each image’s <span class="kbd">H</span> to the mosaic.</li>
      <li><strong>Warp</strong> each image with <span class="kbd">_warp_generic</span> → <span class="kbd">I_k</span> and valid mask.</li>
      <li><strong>Alpha design</strong>: build a center-high, edge-low falloff via <span class="kbd">make_alpha_falloff(h,w,power)</span>, then warp that alpha (bilinear) into the mosaic frame.</li>
      <li><strong>Accumulate</strong> numerator/denominator: <span class="kbd">acc += I_k · α_k</span>, <span class="kbd">wacc += α_k</span>; normalize at the end with an <span class="kbd">ε</span> for stability.</li>
      </ol>
      </div>

      
      <div class="callout"><strong>Procedure</strong>
        <ol>
          <li>Predict final mosaic bounds by transforming every source corner, allocate canvas.</li>
          <li>Inverse-warp each image into the mosaic frame → <span class="kbd">(I_k, α_k)</span>.</li>
          <li>Feather: <span class="kbd">I = (∑ α_k·I_k) / (∑ α_k + ε)</span>; design <span class="kbd">α_k</span> to be high in the center and fall to 0 at borders.</li>
        </ol>
      </div>

      <!-- Mosaic 1 -->
      <h3>Mosaic 1</h3>
      <div class="gallery">
        <figure class="figure"><img src="p3/i11.jpeg" alt="M1 source A"/><figcaption class="figcap">Source A</figcaption></figure>
        <figure class="figure"><img src="p3/i12.jpg" alt="M1 source B"/><figcaption class="figcap">Source B</figcaption></figure>
        <figure class="figure"><img src="p3/mosaic_i11_i12_feather.png" alt="M1 feather blend"/><figcaption class="figcap">Feathered blend.</figcaption></figure>
      </div>

      <!-- Mosaic 2 -->
      <h3>Mosaic 2</h3>
      <div class="gallery">
        <figure class="figure"><img src="p3/i21.jpeg" alt="M2 source A"/><figcaption class="figcap">Source A</figcaption></figure>
        <figure class="figure"><img src="p3/i22.jpg" alt="M2 source B"/><figcaption class="figcap">Source B</figcaption></figure>
        <figure class="figure"><img src="p3/mosaic_i21_i22_feather.png" alt="M2 feather blend"/><figcaption class="figcap">Feathered blend.</figcaption></figure>
      </div>

      <!-- Mosaic 3 -->
      <h3>Mosaic 3</h3>
      <div class="gallery">
        <figure class="figure"><img src="p3/i31.jpg" alt="M3 source A"/><figcaption class="figcap">Source A</figcaption></figure>
        <figure class="figure"><img src="p3/i32.jpg" alt="M3 source B"/><figcaption class="figcap">Source B</figcaption></figure>
        <figure class="figure"><img src="p3/mosaic_i31_i32_feather.png" alt="M3 feather blend"/><figcaption class="figcap">Feathered blend.</figcaption></figure>
      </div>

    </section>


<!-- ===================================================== -->
<!-- ====================== PART 3B ====================== -->
<!-- ===================================================== -->


<section class="section" id="b-overview">
<h1 class="proj-title">Part 3B — Autostitching Pipeline (Harris → ANMS → 8×8 descriptors → Lowe ratio → 4‑pt RANSAC)</h1>
<p>This section documents the <em>implemented</em> pipeline in my code (see <span class="kbd">partB.py</span>): detect corners, thin them with ANMS, build axis‑aligned MOPS‑style descriptors, match with the Lowe ratio test, and estimate a robust homography with 4‑point RANSAC. Mosaics are then produced using the Part A warping/blending.</p>
<div class="callout"><strong>Parameters I actually use</strong>
<ul>
<li><strong>Detector:</strong> <span class="kbd">harris_response(k=0.04, sigma=1.5)</span> → <span class="kbd">harris_nms(thresh_rel=0.01, nms_window=3)</span> → <span class="kbd">anms(c_robust=0.9, num_points≈800)</span>.</li>
<li><strong>Descriptor:</strong> <span class="kbd">extract_mops_descriptors(window_size=40, out_size=8, blur_sigma=1.0, margin=20)</span>, bilinear sampled grid; bias/gain normalization per patch.</li>
<li><strong>Matching:</strong> SSD distances, <span class="kbd">ratio_thresh = 0.72</span> with <span class="kbd">mutual_best=True</span>.</li>
<li><strong>Model:</strong> <span class="kbd">ransac_homography_4pt(pixel_thresh=3.0, max_iters=5000, confidence=0.995)</span> with symmetric transfer error; homographies fit by normalized DLT via <span class="kbd">partA.computeH</span>.</li>
<li><strong>Mosaic:</strong> <span class="kbd">partA.mosaic_feather(images=[B,A], H_to_mosaic=[I, H], mode="bilinear", alpha_power=1.4)</span>.</li>
</ul>
</div>
</section>


<!-- ====================== B.1 ====================== -->
<section class="section" id="b1">
<h2>B.1 — Harris Corner Detection (+ ANMS)</h2>
<p>I implement Harris corners followed by Adaptive Non‑Maximal Suppression to obtain a spatially uniform set of strong and well‑distributed interest points.</p>
<div class="callout"><strong>Implementation</strong>
<ol>
<li><strong>Gradients:</strong> 3×3 Sobel filters (<span class="kbd">Kx</span>, <span class="kbd">Ky=Kx^T</span>) on grayscale in [0,1].</li>
<li><strong>Structure tensor:</strong> blur <span class="kbd">I_x^2, I_y^2, I_x I_y</span> with Gaussian <span class="kbd">σ=1.5</span> via <span class="kbd">_gaussian_blur</span>.</li>
<li><strong>Harris score:</strong> <span class="kbd">R = det(M) − k·trace(M)^2</span> with <span class="kbd">k=0.04</span>.</li>
<li><strong>Local NMS:</strong> threshold at <span class="kbd">1% of max(R)</span> then take 3×3 local maxima (<span class="kbd">harris_nms(thresh_rel=0.01, nms_window=3)</span>).</li>
<li><strong>ANMS:</strong> radius to stronger corners where <span class="kbd">s_j &gt; 0.9·s_i</span> (<span class="kbd">c_robust=0.9</span>); keep largest radii (<span class="kbd">num_points≈800</span> for the pipeline; 500 for compact displays).</li>
</ol>
</div>


<div class="gallery">
<figure class="figure"><img src="p3/harris_corners_no_anms.png" alt="B.1 Harris corners (pre‑ANMS)"/><figcaption class="figcap">Harris corners (pre‑ANMS). <em>Generated via</em> <span class="kbd">overlay_points(...)</span>.</figcaption></figure>
<figure class="figure"><img src="p3/harris_corners_with_anms.png" alt="B.1 ANMS corners"/><figcaption class="figcap">Corners after ANMS (≈K up to 800). <em>overlay</em>.</figcaption></figure>
</div>
</section>

<!-- ====================== B.2 ====================== -->
<section class="section" id="b2">
<h2>B.2 — Feature Descriptor Extraction (8×8 from 40×40)</h2>
<p>I implement axis‑aligned MOPS‑style descriptors. For each ANMS corner, I sample a <strong>40×40</strong> window (bilinear), lightly blur, downsample to <strong>8×8</strong>, and bias/gain‑normalize to zero mean and unit variance.</p>


<div class="callout"><strong>Implementation details</strong>
<ol>
<li><strong>Pre‑blur:</strong> convert to grayscale in [0,1], then <span class="kbd">blur_sigma = 1.0</span>.</li>
<li><strong>Sampling grid:</strong> built by <span class="kbd">_make_descriptor_grid</span> with step <span class="kbd">40/8 = 5</span> px around the center; bilinear via <span class="kbd">_bilinear_sample_grid</span>.</li>
<li><strong>Border policy:</strong> require full support: <span class="kbd">margin=20</span> px, so only in‑bounds points are kept.</li>
<li><strong>Normalization:</strong> per‑patch <span class="kbd">(d−mean)/std</span> with <span class="kbd">ε=1e‑6</span>; descriptors are <span class="kbd">64‑D float32</span>.</li>
</ol>
</div>

<div class="figure figure-center" style="max-width: 980px;">
  <img src="p3/descriptors_grid.png" alt="B.2 patch montage"/>
  <figcaption class="figcap">Montage of 8×8 patches (display‑normalized).</figcaption>
</div>


<div class="figure">
<pre class="codeblock"><code># Key call actually used in code
extract_mops_descriptors(
im, points_xy,
window_size=40, out_size=8,
blur_sigma=1.0, margin=20,
return_patches=True/False
)</code></pre>
<figcaption class="figcap">Descriptor API (as used).</figcaption>
</div>
</section>


<!-- ====================== B.3 ====================== -->
<section class="section" id="b3">
<h2>B.3 — Feature Matching (Lowe Ratio Test)</h2>
<p>I show the <em>matched features</em> (lines between corresponding points) for <strong>three</strong> image pairs using SSD + Lowe ratio (τ = 0.72) with mutual best.</p>


<div class="callout"><strong>Implementation</strong>
<ol>
<li><strong>Distances:</strong> vectorized SSD via <span class="kbd">||a||^2 + ||b||^2 − 2 a·b</span> to form a dense cost matrix.</li>
<li><strong>Ratio test:</strong> for each descriptor in image A, find <span class="kbd">j₁, j₂</span> in image B; accept if <span class="kbd">d₁/(d₂+ε) &lt; 0.72</span>.</li>
<li><strong>Mutual best:</strong> require that <span class="kbd">j₁</span>'s best partner in A is that same feature.</li>
<li><strong>Ranking:</strong> keep matches sorted by ascending ratio.</li>
</ol>
</div>

<div class="figure figure-center" style="max-width: 1040px;">
<img src="p3/matches_im11_im12.png" alt="B.3 matches — pair 1"/>
<figcaption class="figcap">Pair&nbsp;1 — matched features.</figcaption>
</div>
<div class="figure figure-center" style="max-width: 1040px; margin-top: 14px;">
<img src="p3/matches_im21_im22.png" alt="B.3 matches — pair 2"/>
<figcaption class="figcap">Pair&nbsp;2 — matched features.</figcaption>
</div>
<div class="figure figure-center" style="max-width: 1040px; margin-top: 14px;">
<img src="p3/matches_im31_im32.png" alt="B.3 matches — pair 3"/>
<figcaption class="figcap">Pair&nbsp;3 — matched features.</figcaption>
</div>

</section>

<!-- ====================== B.4 ====================== -->
<section class="section" id="b4">
<h2>B.4 — RANSAC for Robust Homography + Automatic Mosaics</h2>
<p>From tentative matches I robustly estimate a homography with a 4‑point RANSAC and symmetric transfer error. The final <span class="kbd">H</span> is re‑fit on inliers via normalized DLT, and I stitch using the same feathered blending as Part A.</p>


<div class="callout"><strong>RANSAC loop</strong>
<ol>
<li>Repeat up to <span class="kbd">max_iters = 5000</span> with RNG seeds set for reproducibility; sample 4 correspondences uniformly.</li>
<li>Fit <span class="kbd">H</span> with <span class="kbd">partA.computeH(use_normalized_dlt=True)</span>.</li>
<li>Score inliers by <em>symmetric</em> transfer error: forward <span class="kbd">im1→im2</span> plus backward <span class="kbd">im2→im1</span> using <span class="kbd">partA.reprojection_errors</span>; accept if <span class="kbd">error &lt; 3 px</span>.</li>
<li>Adapt iterations using <span class="kbd">N = log(1−p)/log(1−w^s)</span> with <span class="kbd">p=0.995</span>, <span class="kbd">s=4</span>, and the current inlier rate <span class="kbd">w</span>.</li>
<li>Refit on all inliers for <span class="kbd">H*</span>; compute final inlier mask.</li>
</ol>
</div>


<div class="answer">
<div class="label">Final settings used</div>
<ul>
<li><strong>Pixel threshold:</strong> ε = 3.0 px</li>
<li><strong>Max iters:</strong> 5000 (adaptive early‑stop to reach 99.5% confidence)</li>
<li><strong>Error metric:</strong> symmetric forward+backward reprojection</li>
<li><strong>Blending:</strong> feather, <span class="kbd">alpha_power=1.4</span></li>
</ul>
</div>


<h3>Manual vs Automatic Stitch</h3>
  <div class="figure figure-center" style="max-width: 980px;">
<img src="p3/compare_manual_vs_auto.png" alt="B.4 manual mosaic"/><figcaption class="figcap">Manual correspondences vs Automatic pipeline</figcaption>
</div>

<div class="answer">
<div class="label">Comparison: what changes and why</div>
<ul>
<li><strong>Geometric alignment.</strong> The automatic result slightly improves line consistency at long straight edges (roofline, balcony rail posts). With dozens–hundreds of inliers, RANSAC averages out individual click noise that can bias a manual 8–12 point fit.</li>
<li><strong>Seam visibility.</strong> Feathering is identical across both, but the automatic <span class="kbd">H</span> tends to place the seam where parallax is lower, so subtle tone/edge discontinuities are reduced compared to the manual fit.</li>
<li><strong>Global warp.</strong> Small differences in homography (scale/shear) change the black background wedges at the canvas borders. The automatic estimate often yields a slightly more rectangular panorama footprint; any remaining wedges can be cropped identically for both.</li>
<li><strong>Robustness to bad pairs.</strong> Manual clicks near low‑texture sky/building boundaries can introduce outliers. The automatic pipeline rejects such pairs via the Lowe ratio (τ≈0.72) and a symmetric transfer error threshold (ε≈3 px) inside RANSAC.</li>
<li><strong>Effort vs. runtime.</strong> Manual requires careful clicking and re‑doing on mistakes; automatic is click‑free and runs in seconds on CPU for ~800 corners per image (dense SSD, ratio test, then RANSAC).</li>
<li><strong>When manual can look better.</strong> If scenes have strong parallax (camera translated), repeated textures, or large exposure shifts, hand‑picked, well‑distributed points on the dominant plane can outperform blind matching. In those cases, tightening the ratio threshold (e.g., 0.65), lowering ε to 2–2.5 px, or increasing ANMS count can help the automatic pipeline.</li>
</ul>
</div>


<h3>Automatic Mosaics</h3>
<div class="gallery">
<figure class="figure"><img src="p3/mosaic_auto_pair1.png" alt="B.4 auto mosaic 1"/><figcaption class="figcap">Auto mosaic #1.</figcaption></figure>
<figure class="figure"><img src="p3/mosaic_auto_pair2.png" alt="B.4 auto mosaic 2"/><figcaption class="figcap">Auto mosaic #2.</figcaption></figure>
</div>
<div class="gallery">
<figure class="figure"><img src="p3/mosaic_auto_pair3.png" alt="B.4 auto mosaic 3"/><figcaption class="figcap">Auto mosaic #3.</figcaption></figure>
</div>
</section>
    
  </main>

  <footer class="container footer">
    <div>Project 3 · CS180/280A</div>
  </footer>
</body>
</html>
