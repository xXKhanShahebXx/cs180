<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Project 5A — Diffusion Models, Inpainting & Optical Illusions</title>
  <meta name="theme-color" content="#0f1226"/>
  <link rel="stylesheet" href="assets/style.css?v=2"/>
</head>
<body>
  <!-- Top breadcrumb -->
  <nav class="topbar">
    <div class="container" style="padding:10px 24px">
      <div class="breadcrumb">
        <a href="index.html">← Back to Home</a>
        <span>·</span>
        <span>Project 5A — Diffusion Models</span>
      </div>
    </div>
  </nav>

  <main class="container">

    <!-- ====================== OVERVIEW ====================== -->
    <section class="section" id="overview">
      <h1 class="proj-title">Project 5A — Diffusion Models, Sampling, Inpainting & Illusions</h1>
      <p class="proj-sub">
        In Part A, I explore diffusion models through the DeepFloyd IF pipeline. I implement the forward noising process,
        write custom sampling/denoising loops, and then adapt these loops for image-to-image editing, inpainting, and
        classic diffusion-based optical illusions (visual anagrams + hybrid images).
      </p>

      <div class="callout">
        <strong>High-level structure</strong>
        <ul>
          <li><strong>Part 0:</strong> Setup, prompt embeddings, and baseline text-to-image samples.</li>
          <li><strong>Part 1:</strong> Forward process + classical denoising, one-step denoising, iterative denoising,
            CFG sampling, SDEdit, inpainting, text-conditional edits.</li>
          <li><strong>Part 1.8–1.9:</strong> Visual Anagrams and Factorized Diffusion Hybrid Images.</li>
          <li><strong>(Optional):</strong> Extra explorations if applicable for the course version.</li>
        </ul>
      </div>

      <div class="callout">
        <strong>Jump to</strong>
        <div class="cta-row" style="margin-top:8px">
          <a class="btn secondary" href="#p0-setup">Part 0</a>
          <a class="btn secondary" href="#p11-forward">1.1</a>
          <a class="btn secondary" href="#p14-iter">1.4</a>
          <a class="btn secondary" href="#p16-cfg">1.6</a>
          <a class="btn secondary" href="#p17-sdedit">1.7</a>
          <a class="btn secondary" href="#p18-anagrams">1.8</a>
          <a class="btn secondary" href="#p19-hybrids">1.9</a>
        </div>
      </div>
    </section>

<!-- ====================== PART 0 ====================== -->
<section class="section" id="p0-setup">
  <h1 class="proj-title">Part 0 — Setup & Text-to-Image with DeepFloyd IF</h1>
  <p>
    I gained access to DeepFloyd IF via Hugging Face, generated prompt embeddings, and sampled Stage I/II
    text-to-image outputs. I explored a small prompt set that I later reused for visual anagrams and
    hybrid images.
  </p>

  <div class="callout">
    <strong>Prompt pool used for embeddings</strong>
    <ul>
      <li>a high quality picture</li>
      <li>an oil painting of a snowy mountain village</li>
      <li>a photo of the amalfi coast</li>
      <li>a photo of a man</li>
      <li>a photo of a hipster barista</li>
      <li>a photo of a dog</li>
      <li>an oil painting of people around a campfire</li>
      <li>an oil painting of an old man</li>
      <li>a lithograph of waterfalls</li>
      <li>a lithograph of a skull</li>
      <li>a man wearing a hat</li>
      <li>a high quality photo</li>
      <li>a rocket ship</li>
      <li>a pencil</li>
    </ul>
  </div>

  <div class="answer">
    <div class="label">Reproducibility</div>
    <ul>
      <li><strong>Seed used for Part A:</strong> <span class="kbd">180</span></li>
      <li>
        I keep this seed fixed for all subsequent sections to make comparisons across
        denoising schedules, CFG, and editing tasks consistent.
      </li>
    </ul>
  </div>

  <!-- ====================== num_inference_steps = 20 ====================== -->
  <h2>Baseline generations — num_inference_steps = 20</h2>
  <p>
    Below are three selected prompts at <span class="kbd">20</span> inference steps. For each prompt,
    I show both DeepFloyd stages side-by-side.
  </p>

  <!-- Set 1 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5a/p0_steps20_set1_stage1.png" alt="Steps 20 set 1 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 1 · Stage I</strong> (64×64) — <em>Selected Prompt 1</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5a/p0_steps20_set1_stage2.png" alt="Steps 20 set 1 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 1 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <!-- Set 2 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5a/p0_steps20_set2_stage1.png" alt="Steps 20 set 2 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 2 · Stage I</strong> (64×64) — <em>Selected Prompt 2</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5a/p0_steps20_set2_stage2.png" alt="Steps 20 set 2 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 2 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <!-- Set 3 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5a/p0_steps20_set3_stage1.png" alt="Steps 20 set 3 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 3 · Stage I</strong> (64×64) — <em>Selected Prompt 3</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5a/p0_steps20_set3_stage2.png" alt="Steps 20 set 3 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 3 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <!-- ====================== num_inference_steps = 50 ====================== -->
  <h2>Baseline generations — num_inference_steps = 50</h2>
  <p>
    I repeat the same three prompts with <span class="kbd">50</span> inference steps to compare
    prompt alignment, sharpness, and global coherence against the 20-step results.
  </p>

  <!-- Set 1 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5a/p0_steps50_set1_stage1.png" alt="Steps 50 set 1 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 1 · Stage I</strong> (64×64) — <em>Selected Prompt 1</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5a/p0_steps50_set1_stage2.png" alt="Steps 50 set 1 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 1 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <!-- Set 2 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5a/p0_steps50_set2_stage1.png" alt="Steps 50 set 2 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 2 · Stage I</strong> (64×64) — <em>Selected Prompt 2</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5a/p0_steps50_set2_stage2.png" alt="Steps 50 set 2 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 2 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <!-- Set 3 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5a/p0_steps50_set3_stage1.png" alt="Steps 50 set 3 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 3 · Stage I</strong> (64×64) — <em>Selected Prompt 3</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5a/p0_steps50_set3_stage2.png" alt="Steps 50 set 3 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 3 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <div class="answer">
    <div class="label">Quick reflection (placeholder)</div>
    <ul>
      <li><strong>20 steps:</strong> <em>brief note on speed/quality trade-off</em>.</li>
      <li><strong>50 steps:</strong> <em>brief note on improvements in structure, texture, or prompt fidelity</em>.</li>
      <li><strong>Stage I vs II:</strong> <em>comment on how upsampling refines details and reduces artifacts</em>.</li>
    </ul>
  </div>
</section>

    <!-- ====================== PART 1 ====================== -->
    <section class="section" id="p1-overview">
      <h1 class="proj-title">Part 1 — Sampling Loops & Diffusion-Based Editing</h1>
      <p>
        In this part I implement core diffusion mechanics: the forward process, one-step denoising,
        and accelerated iterative denoising using a strided schedule. I then extend the same loop with
        classifier-free guidance (CFG) and apply it to sampling, SDEdit-style image-to-image translation,
        and inpainting.
      </p>
    </section>

    <!-- ====================== 1.1 Forward Process ====================== -->
    <section class="section" id="p11-forward">
      <h2>Part 1.1 — Implementing the Forward Process</h2>
      <p>
        I implement <span class="kbd">forward(im, t)</span> using the cumulative noise schedule
        <span class="kbd">alphas_cumprod</span>. Starting from a clean Campanile image, I generate
        progressively noisier versions at increasing timesteps.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Forward function implementation.</li>
          <li>Noisy Campanile at <span class="kbd">t = 250, 500, 750</span>.</li>
        </ul>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/campanile_clean.png" alt="Original Campanile"/>
          <figcaption class="figcap">Original 64×64 Campanile.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/campanile_t250.png" alt="Noisy Campanile at t=250"/>
          <figcaption class="figcap">Forward process at <span class="kbd">t=250</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/campanile_t500.png" alt="Noisy Campanile at t=500"/>
          <figcaption class="figcap">Forward process at <span class="kbd">t=500</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/campanile_t750.png" alt="Noisy Campanile at t=750"/>
          <figcaption class="figcap">Forward process at <span class="kbd">t=750</span>.</figcaption>
        </figure>
      </div>
    </section>

    <!-- ====================== 1.2 Classical Denoising ====================== -->
    <section class="section" id="p12-gaussian">
      <h2>Part 1.2 — Classical Denoising with Gaussian Blur</h2>
      <p>
        I attempt to denoise the same noisy Campanile images using Gaussian blur. As expected, this baseline
        struggles to recover structure at high noise levels compared to a learned diffusion prior.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Side-by-side noisy vs Gaussian-denoised results for <span class="kbd">t = 250, 500, 750</span>.</li>
        </ul>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/gauss_t250_pair.png" alt="Noisy and Gaussian blurred at t=250"/>
          <figcaption class="figcap">Noisy vs Gaussian blur at <span class="kbd">t=250</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/gauss_t500_pair.png" alt="Noisy and Gaussian blurred at t=500"/>
          <figcaption class="figcap">Noisy vs Gaussian blur at <span class="kbd">t=500</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/gauss_t750_pair.png" alt="Noisy and Gaussian blurred at t=750"/>
          <figcaption class="figcap">Noisy vs Gaussian blur at <span class="kbd">t=750</span>.</figcaption>
        </figure>
      </div>
    </section>

    <!-- ====================== 1.3 One-Step Denoising ====================== -->
    <section class="section" id="p13-onestep">
      <h2>Part 1.3 — One-Step Denoising with the Pretrained UNet</h2>
      <p>
        Using the Stage I UNet conditioned on timestep and a weak prompt embedding
        (<span class="kbd">"a high quality photo"</span>), I estimate the Gaussian noise and recover an
        approximation of the original Campanile in a single denoising step.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Original, noisy, and one-step denoised comparisons for <span class="kbd">t = 250, 500, 750</span>.</li>
        </ul>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/onestep_t250_triplet.png" alt="Original, noisy, and one-step denoise at t=250"/>
          <figcaption class="figcap">One-step denoising at <span class="kbd">t=250</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/onestep_t500_triplet.png" alt="Original, noisy, and one-step denoise at t=500"/>
          <figcaption class="figcap">One-step denoising at <span class="kbd">t=500</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/onestep_t750_triplet.png" alt="Original, noisy, and one-step denoise at t=750"/>
          <figcaption class="figcap">One-step denoising at <span class="kbd">t=750</span>.</figcaption>
        </figure>
      </div>
    </section>

    <!-- ====================== 1.4 Iterative Denoising ====================== -->
    <section class="section" id="p14-iter">
      <h2>Part 1.4 — Iterative Denoising with a Strided Schedule</h2>
      <p>
        I construct <span class="kbd">strided_timesteps</span> starting at 990 with stride 30 and implement
        <span class="kbd">iterative_denoise</span>. This produces significantly better reconstructions than a
        single-step estimate, especially at moderate noise levels.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Strided timestep schedule initialization.</li>
          <li>Denoising loop visualization (every 5th step).</li>
          <li>Comparison to one-step denoising and Gaussian blur.</li>
        </ul>
      </div>

      <div class="figure figure-center">
        <img src="p5a/iter_progress_strip.png" alt="Iterative denoising progression"/>
        <figcaption class="figcap">
          Iterative denoising progression (every 5th step) starting from a noisy Campanile at
          <span class="kbd">i_start = 10</span>.
        </figcaption>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/iter_final.png" alt="Final iteratively denoised Campanile"/>
          <figcaption class="figcap">Final iterative reconstruction.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/iter_onestep_compare.png" alt="One-step comparison"/>
          <figcaption class="figcap">One-step reconstruction (worse).</figcaption>
        </figure>
      </div>

      <div class="figure figure-center" style="max-width: 740px;">
        <img src="p5a/iter_gauss_compare.png" alt="Gaussian blur comparison"/>
        <figcaption class="figcap">Gaussian blur baseline for reference.</figcaption>
      </div>
    </section>

    <!-- ====================== 1.5 Sampling from Noise ====================== -->
    <section class="section" id="p15-sampling">
      <h2>Part 1.5 — Diffusion Model Sampling (Unconditional-ish)</h2>
      <p>
        By setting <span class="kbd">i_start = 0</span> and initializing with pure Gaussian noise, I use the
        iterative denoising loop to generate images from scratch using the weak prompt
        <span class="kbd">"a high quality photo"</span>.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Five sampled images without CFG.</li>
        </ul>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/sample_nocfg_1.png" alt="Sample 1 without CFG"/>
          <figcaption class="figcap">Sample 1 (no CFG).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sample_nocfg_2.png" alt="Sample 2 without CFG"/>
          <figcaption class="figcap">Sample 2 (no CFG).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sample_nocfg_3.png" alt="Sample 3 without CFG"/>
          <figcaption class="figcap">Sample 3 (no CFG).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sample_nocfg_4.png" alt="Sample 4 without CFG"/>
          <figcaption class="figcap">Sample 4 (no CFG).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sample_nocfg_5.png" alt="Sample 5 without CFG"/>
          <figcaption class="figcap">Sample 5 (no CFG).</figcaption>
        </figure>
      </div>
    </section>

    <!-- ====================== 1.6 CFG ====================== -->
    <section class="section" id="p16-cfg">
      <h2>Part 1.6 — Classifier-Free Guidance (CFG)</h2>
      <p>
        I implement <span class="kbd">iterative_denoise_cfg</span> by combining conditional and unconditional
        noise estimates. With a positive CFG scale, generation quality improves significantly at the cost of
        reduced diversity.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>CFG-enabled denoising loop implementation.</li>
          <li>Five samples of <span class="kbd">"a high quality photo"</span> using CFG.</li>
        </ul>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/sample_cfg_1.png" alt="Sample 1 with CFG"/>
          <figcaption class="figcap">Sample 1 (CFG).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sample_cfg_2.png" alt="Sample 2 with CFG"/>
          <figcaption class="figcap">Sample 2 (CFG).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sample_cfg_3.png" alt="Sample 3 with CFG"/>
          <figcaption class="figcap">Sample 3 (CFG).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sample_cfg_4.png" alt="Sample 4 with CFG"/>
          <figcaption class="figcap">Sample 4 (CFG).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sample_cfg_5.png" alt="Sample 5 with CFG"/>
          <figcaption class="figcap">Sample 5 (CFG).</figcaption>
        </figure>
      </div>
    </section>

    <!-- ====================== 1.7 SDEdit + Edits ====================== -->
    <section class="section" id="p17-sdedit">
      <h2>Part 1.7 — Image-to-Image Translation (SDEdit)</h2>
      <p>
        Using CFG from this point onward, I apply SDEdit by adding noise to a real image and then projecting it
        back to the natural image manifold. I vary the starting index
        <span class="kbd">i_start ∈ {1, 3, 5, 7, 10, 20}</span>
        to control edit strength.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Campanile edits across the required starting indices using the prompt <span class="kbd">"a high quality photo"</span>.</li>
          <li>Two additional personal images edited with the same procedure.</li>
        </ul>
      </div>

      <div class="figure figure-center">
        <img src="p5a/sdedit_campanile_grid.png" alt="SDEdit Campanile results grid"/>
        <figcaption class="figcap">
          SDEdit on the Campanile across increasing <span class="kbd">i_start</span> values (lower = stronger edits).
        </figcaption>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/sdedit_own1_grid.png" alt="SDEdit on personal image 1"/>
          <figcaption class="figcap">SDEdit on my image #1.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sdedit_own2_grid.png" alt="SDEdit on personal image 2"/>
          <figcaption class="figcap">SDEdit on my image #2.</figcaption>
        </figure>
      </div>

      <!-- 1.7.1 -->
      <div class="callout">
        <strong>1.7.1 — Non-photorealistic inputs</strong>
        <p style="margin:8px 0 0">
          I also apply the same procedure to one web image and two hand-drawn sketches to demonstrate how diffusion
          can project rough or stylized inputs onto a more realistic manifold.
        </p>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/sdedit_web_grid.png" alt="SDEdit on a web image"/>
          <figcaption class="figcap">Web image projected across noise levels.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sdedit_draw1_grid.png" alt="SDEdit on hand-drawn image 1"/>
          <figcaption class="figcap">Hand-drawn sketch #1 → diffusion edits.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sdedit_draw2_grid.png" alt="SDEdit on hand-drawn image 2"/>
          <figcaption class="figcap">Hand-drawn sketch #2 → diffusion edits.</figcaption>
        </figure>
      </div>

      <!-- 1.7.2 -->
      <div class="callout">
        <strong>1.7.2 — Inpainting</strong>
        <p style="margin:8px 0 0">
          I implement inpainting by forcing pixels outside the mask to match the appropriately noised original
          image at each timestep.
        </p>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/inpaint_campanile_input.png" alt="Campanile input for inpainting"/>
          <figcaption class="figcap">Input Campanile.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/inpaint_campanile_mask.png" alt="Inpainting mask"/>
          <figcaption class="figcap">Binary edit mask.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/inpaint_campanile_result.png" alt="Campanile inpainted result"/>
          <figcaption class="figcap">Inpainted Campanile result.</figcaption>
        </figure>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/inpaint_own1_triplet.png" alt="Personal inpainting example 1"/>
          <figcaption class="figcap">My image #1 inpainting (input/mask/result).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/inpaint_own2_triplet.png" alt="Personal inpainting example 2"/>
          <figcaption class="figcap">My image #2 inpainting (input/mask/result).</figcaption>
        </figure>
      </div>

      <!-- 1.7.3 -->
      <div class="callout">
        <strong>1.7.3 — Text-conditional image-to-image</strong>
        <p style="margin:8px 0 0">
          Finally, I replace the generic prompt with my own prompts to steer edits toward specific semantic changes
          while still preserving underlying structure at lower noise levels.
        </p>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/text_edit_campanile_grid.png" alt="Text-guided edits on Campanile"/>
          <figcaption class="figcap">Campanile edits with a custom prompt across <span class="kbd">i_start</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/text_edit_own1_grid.png" alt="Text-guided edits on personal image 1"/>
          <figcaption class="figcap">My image #1 with text-guided edits.</figcaption>
        </figure>
      </div>
    </section>

    <!-- ====================== 1.8 Visual Anagrams ====================== -->
    <section class="section" id="p18-anagrams">
      <h2>Part 1.8 — Visual Anagrams (Flip Illusions)</h2>
      <p>
        I implement visual anagrams by denoising an image with two prompts simultaneously: one on the original image
        and one on a vertically flipped copy. Averaging the aligned noise estimates produces a single image that
        changes interpretation when flipped upside down.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Implemented <span class="kbd">visual_anagrams</span> function.</li>
          <li>Two flip illusions with paired prompts.</li>
        </ul>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/anagram1_upright.png" alt="Visual anagram 1 upright"/>
          <figcaption class="figcap">Illusion #1 (upright) — <em>Prompt A</em>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/anagram1_flipped.png" alt="Visual anagram 1 flipped"/>
          <figcaption class="figcap">Illusion #1 (flipped) — <em>Prompt B</em>.</figcaption>
        </figure>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/anagram2_upright.png" alt="Visual anagram 2 upright"/>
          <figcaption class="figcap">Illusion #2 (upright) — <em>Prompt A</em>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/anagram2_flipped.png" alt="Visual anagram 2 flipped"/>
          <figcaption class="figcap">Illusion #2 (flipped) — <em>Prompt B</em>.</figcaption>
        </figure>
      </div>
    </section>

    <!-- ====================== 1.9 Hybrid Images ====================== -->
    <section class="section" id="p19-hybrids">
      <h2>Part 1.9 — Hybrid Images with Factorized Diffusion</h2>
      <p>
        To create diffusion-based hybrid images, I combine low-frequency structure from one prompt’s noise estimate
        with high-frequency detail from another. This mirrors the classic hybrid image idea from Project 2,
        but performed in the noise-prediction space of a diffusion model.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Implemented <span class="kbd">make_hybrids</span> function.</li>
          <li>Two hybrid results using paired prompts.</li>
        </ul>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/hybrid1.png" alt="Hybrid image 1"/>
          <figcaption class="figcap">Hybrid #1 — <em>Prompt A (low-pass)</em> + <em>Prompt B (high-pass)</em>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/hybrid2.png" alt="Hybrid image 2"/>
          <figcaption class="figcap">Hybrid #2 — <em>Prompt A (low-pass)</em> + <em>Prompt B (high-pass)</em>.</figcaption>
        </figure>
      </div>

      <div class="answer">
        <div class="label">Perception note</div>
        <p>
          Up close, the high-frequency concept tends to dominate; from farther away or at smaller display sizes,
          the low-frequency concept becomes more apparent.
        </p>
      </div>
    </section>

  </main>

  <footer class="container footer">
    <div>Project 5A · CS180/280A</div>
  </footer>
</body>
</html>
