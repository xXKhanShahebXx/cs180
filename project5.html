<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Project 5 — Diffusion Models & Flow Matching</title>
  <meta name="theme-color" content="#0f1226"/>
  <link rel="stylesheet" href="assets/style.css?v=2"/>
</head>
<body>
  <!-- Top breadcrumb -->
  <nav class="topbar">
    <div class="container" style="padding:10px 24px">
      <div class="breadcrumb">
        <a href="index.html">← Back to Home</a>
        <span>·</span>
        <span>Project 5 — Fun With Diffusion Models</span>
      </div>
    </div>
  </nav>

  <main class="container">

    <!-- ====================== OVERVIEW ====================== -->
    <section class="section" id="overview">
      <h1 class="proj-title">Project 5A — Diffusion Models, Sampling, Inpainting & Illusions</h1>
      <p class="proj-sub">
        In Part A, I explore diffusion models through the DeepFloyd IF pipeline. I implement the forward noising process,
        write custom sampling/denoising loops, and then adapt these loops for image-to-image editing, inpainting, and
        classic diffusion-based optical illusions (visual anagrams + hybrid images).
      </p>

      <div class="callout">
        <strong>High-level structure</strong>
        <ul>
          <li><strong>Part 0:</strong> Setup, prompt embeddings, and baseline text-to-image samples.</li>
          <li><strong>Part 1:</strong> Forward process + classical denoising, one-step denoising, iterative denoising,
            CFG sampling, SDEdit, inpainting, text-conditional edits.</li>
          <li><strong>Part 1.8–1.9:</strong> Visual Anagrams and Factorized Diffusion Hybrid Images.</li>
          <li><strong>(Optional):</strong> Extra explorations if applicable for the course version.</li>
        </ul>
      </div>

      <div class="callout">
        <strong>Jump to</strong>
        <div class="cta-row" style="margin-top:8px">
          <a class="btn secondary" href="#p0-setup">A.0</a>
          <a class="btn secondary" href="#p11-forward">A.1.1</a>
          <a class="btn secondary" href="#p14-iter">A.1.4</a>
          <a class="btn secondary" href="#p16-cfg">A.1.6</a>
          <a class="btn secondary" href="#p17-sdedit">A.1.7</a>
          <a class="btn secondary" href="#p18-anagrams">A.1.8</a>
          <a class="btn secondary" href="#p19-hybrids">A.1.9</a>
        </div>
      </div>
    </section>

<!-- ====================== PART 0 ====================== -->
<section class="section" id="p0-setup">
  <h1 class="proj-title">Part 0 — Setup & Text-to-Image with DeepFloyd IF</h1>
  <p>
    I gained access to DeepFloyd IF via Hugging Face, generated prompt embeddings, and sampled Stage I/II
    text-to-image outputs. I explored a small prompt set that I later reused for visual anagrams and
    hybrid images.
  </p>

  <div class="callout">
    <strong>Prompt pool used for embeddings</strong>
    <ul>
      <li>a high quality picture</li>
      <li>an oil painting of a snowy mountain village</li>
      <li>a photo of the amalfi coast</li>
      <li>a photo of a man</li>
      <li>a photo of a hipster barista</li>
      <li>a photo of a dog</li>
      <li>an oil painting of people around a campfire</li>
      <li>an oil painting of an old man</li>
      <li>a lithograph of waterfalls</li>
      <li>a lithograph of a skull</li>
      <li>a man wearing a hat</li>
      <li>a high quality photo</li>
      <li>a rocket ship</li>
      <li>a pencil</li>
    </ul>
  </div>

  <div class="answer">
    <div class="label">Reproducibility</div>
    <ul>
      <li><strong>Seed used for Part A:</strong> <span class="kbd">180</span></li>
      <li>
        I keep this seed fixed for all subsequent sections to make comparisons across
        denoising schedules, CFG, and editing tasks consistent.
      </li>
    </ul>
  </div>

  <!-- ====================== num_inference_steps = 20 ====================== -->
  <h2>Baseline generations — num_inference_steps = 20</h2>
  <p>
    Below are three selected prompts at <span class="kbd">20</span> inference steps. For each prompt,
    I show both DeepFloyd stages side-by-side.
  </p>

  <!-- Set 1 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5/old_1_20.png" alt="Steps 20 set 1 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 1 · Stage I</strong> (64×64) — <em>an oil painting of an old man</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/old_2_20.png" alt="Steps 20 set 1 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 1 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <!-- Set 2 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5/mt_1_20.png" alt="Steps 20 set 2 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 2 · Stage I</strong> (64×64) — <em>a photo of a snowy mountain</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/mt_2_20.png" alt="Steps 20 set 2 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 2 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <!-- Set 3 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5/rabbit_1_20.png" alt="Steps 20 set 3 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 3 · Stage I</strong> (64×64) — <em>a photo of a rabbit</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/rabbit_2_20.png" alt="Steps 20 set 3 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 3 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <!-- ====================== num_inference_steps = 50 ====================== -->
  <h2>Baseline generations — num_inference_steps = 50</h2>
  <p>
    I repeat the same three prompts with <span class="kbd">50</span> inference steps to compare
    prompt alignment, sharpness, and global coherence against the 20-step results.
  </p>

  <!-- Set 1 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5/old_1_50.png" alt="Steps 50 set 1 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 1 · Stage I</strong> (64×64) — <em>an oil painting of an old man</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/old_2_50.png" alt="Steps 50 set 1 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 1 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <!-- Set 2 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5/mt_1_50.png" alt="Steps 50 set 2 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 2 · Stage I</strong> (64×64) — <em>a photo of a snowy mountain</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/mt_2_50.png" alt="Steps 50 set 2 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 2 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <!-- Set 3 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5/rabbit_1_50.png" alt="Steps 50 set 3 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 3 · Stage I</strong> (64×64) — <em>a photo of a rabbit</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/rabbit_2_50.png" alt="Steps 50 set 3 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 3 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <div class="answer">
    <div class="label">Quick reflection</div>
    <ul>
    <li><strong>20 steps:</strong> The Stage I results capture the rough semantics of each prompt, but the images can look softer and less stable in composition. Stage II helps, but some minor artifacts and weaker prompt-specific details can remain at this lower step count.</li>
    <li><strong>50 steps:</strong> The generations are noticeably more consistent and better aligned with the text. The “oil painting of an old man” shows clearer painterly structure, while the “snowy mountain” and “rabbit” prompts look more coherent with cleaner edges and more believable texture.</li>
    <li><strong>Stage I vs II:</strong> Stage I (64×64) establishes the coarse layout and color palette, while Stage II (256×256) acts as a refinement pass that adds high-frequency detail and improves the intended style (photo vs. oil painting), making the outputs feel substantially more complete.</li>
    </ul>
  </div>
</section>

    <!-- ====================== PART 1 ====================== -->
    <section class="section" id="p1-overview">
      <h1 class="proj-title">Part 1 — Sampling Loops & Diffusion-Based Editing</h1>
      <p>
        In this part I implement core diffusion mechanics: the forward process, one-step denoising,
        and accelerated iterative denoising using a strided schedule. I then extend the same loop with
        classifier-free guidance (CFG) and apply it to sampling, SDEdit-style image-to-image translation,
        and inpainting.
      </p>
    </section>

    <!-- ====================== 1.1 Forward Process ====================== -->
    <section class="section" id="p11-forward">
      <h2>Part 1.1 — Implementing the Forward Process</h2>
      <p>
        I implement <span class="kbd">forward(im, t)</span> using the cumulative noise schedule
        <span class="kbd">alphas_cumprod</span>. Starting from a clean Campanile image, I generate
        progressively noisier versions at increasing timesteps.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Forward function implementation.</li>
          <li>Noisy Campanile at <span class="kbd">t = 250, 500, 750</span>.</li>
        </ul>
      </div>

      <div class="figure">
<pre class="codeblock"><code>def forward(im, t):
    """
    Args:
      im : torch tensor of size (1, 3, 64, 64) representing the clean image
      t : integer timestep

    Returns:
      im_noisy : torch tensor of size (1, 3, 64, 64) representing the noisy image at timestep t
    """
    with torch.no_grad():
        alpha_t = alphas_cumprod[t].to(im.device)
        epsilon = torch.randn_like(im)
        im_noisy = torch.sqrt(alpha_t) * im + torch.sqrt(1 - alpha_t) * epsilon

    return im_noisy</code></pre>
  <figcaption class="figcap">
    Forward diffusion implementation used to generate noisy versions of the 64×64 Campanile at specific timesteps.
  </figcaption>
</div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5/campanile_clean.png" alt="Original Campanile"/>
          <figcaption class="figcap">Original 64×64 Campanile.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/campanile_t250.png" alt="Noisy Campanile at t=250"/>
          <figcaption class="figcap">Forward process at <span class="kbd">t=250</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/campanile_t500.png" alt="Noisy Campanile at t=500"/>
          <figcaption class="figcap">Forward process at <span class="kbd">t=500</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/campanile_t750.png" alt="Noisy Campanile at t=750"/>
          <figcaption class="figcap">Forward process at <span class="kbd">t=750</span>.</figcaption>
        </figure>
      </div>
    </section>

<!-- ====================== 1.2 Classical Denoising ====================== -->
<section class="section" id="p12-gaussian">
  <h2>Part 1.2 — Classical Denoising with Gaussian Blur</h2>
  <p>
    I denoise the noisy Campanile images from Part 1.1 using Gaussian blur. As expected, this classical baseline
    struggles to recover fine structure at higher noise levels compared to learned diffusion denoisers.
  </p>

  <div class="callout">
    <strong>Deliverables</strong>
    <ul>
      <li>For <span class="kbd">t = 250, 500, 750</span>, show the noisy image and my best Gaussian-denoised result side by side.</li>
    </ul>
  </div>

  <h3>t = 250</h3>
  <div class="gallery">
    <figure class="figure">
      <img src="p5/campanile_t250.png" alt="Noisy Campanile at t=250"/>
      <figcaption class="figcap">Noisy Campanile at <span class="kbd">t=250</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/gauss_t250.png" alt="Gaussian blur denoising at t=250"/>
      <figcaption class="figcap">Gaussian blur denoising at <span class="kbd">t=250</span>.</figcaption>
    </figure>
  </div>

  <h3>t = 500</h3>
  <div class="gallery">
    <figure class="figure">
      <img src="p5/campanile_t500.png" alt="Noisy Campanile at t=500"/>
      <figcaption class="figcap">Noisy Campanile at <span class="kbd">t=500</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/gauss_t500.png" alt="Gaussian blur denoising at t=500"/>
      <figcaption class="figcap">Gaussian blur denoising at <span class="kbd">t=500</span>.</figcaption>
    </figure>
  </div>

  <h3>t = 750</h3>
  <div class="gallery">
    <figure class="figure">
      <img src="p5/campanile_t750.png" alt="Noisy Campanile at t=750"/>
      <figcaption class="figcap">Noisy Campanile at <span class="kbd">t=750</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/gauss_t750.png" alt="Gaussian blur denoising at t=750"/>
      <figcaption class="figcap">Gaussian blur denoising at <span class="kbd">t=750</span>.</figcaption>
    </figure>
  </div>
</section>

<!-- ====================== 1.3 One-Step Denoising ====================== -->
<section class="section" id="p13-onestep">
  <h2>Part 1.3 — One-Step Denoising with the Pretrained UNet</h2>
  <p>
    I use the pretrained Stage I UNet to estimate the noise in each noisy Campanile image, then recover an
    estimate of the original image using the standard diffusion inversion formula. Unlike Gaussian blur,
    the UNet can project the input toward the natural-image manifold, though performance degrades as the
    noise level increases.
  </p>

  <div class="callout">
    <strong>Deliverables</strong>
    <ul>
      <li>For <span class="kbd">t = 250, 500, 750</span>:
        show the original image, the noisy image (from my forward process),
        and the one-step denoised estimate.</li>
    </ul>
  </div>

  <h3>t = 250</h3>
  <div class="gallery" style="grid-template-columns: repeat(3, minmax(0, 1fr));">
    <figure class="figure">
      <img src="p5/campanile_clean.png" alt="Original Campanile"/>
      <figcaption class="figcap">Original Campanile (64×64).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_t250.png" alt="Noisy Campanile at t=250"/>
      <figcaption class="figcap">Noisy Campanile at <span class="kbd">t=250</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/onestep_t250.png" alt="One-step denoised Campanile at t=250"/>
      <figcaption class="figcap">One-step denoised at <span class="kbd">t=250</span>.</figcaption>
    </figure>
  </div>

  <h3>t = 500</h3>
  <div class="gallery" style="grid-template-columns: repeat(3, minmax(0, 1fr));">
    <figure class="figure">
      <img src="p5/campanile_clean.png" alt="Original Campanile"/>
      <figcaption class="figcap">Original Campanile (64×64).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_t500.png" alt="Noisy Campanile at t=500"/>
      <figcaption class="figcap">Noisy Campanile at <span class="kbd">t=500</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/onestep_t500.png" alt="One-step denoised Campanile at t=500"/>
      <figcaption class="figcap">One-step denoised at <span class="kbd">t=500</span>.</figcaption>
    </figure>
  </div>

  <h3>t = 750</h3>
  <div class="gallery" style="grid-template-columns: repeat(3, minmax(0, 1fr));">
    <figure class="figure">
      <img src="p5/campanile_clean.png" alt="Original Campanile"/>
      <figcaption class="figcap">Original Campanile (64×64).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_t750.png" alt="Noisy Campanile at t=750"/>
      <figcaption class="figcap">Noisy Campanile at <span class="kbd">t=750</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/onestep_t750.png" alt="One-step denoised Campanile at t=750"/>
      <figcaption class="figcap">One-step denoised at <span class="kbd">t=750</span>.</figcaption>
    </figure>
  </div>

  <div class="answer">
    <div class="label">Observations</div>
    <ul>
      <li>At <span class="kbd">t=250</span>, the UNet recovers recognizable structure and sharper edges than Gaussian blur.</li>
      <li>At <span class="kbd">t=500</span>, the estimate is still plausible but loses finer detail.</li>
      <li>At <span class="kbd">t=750</span>, one-step denoising struggles, motivating the iterative procedure in Part 1.4.</li>
    </ul>
  </div>
</section>


<!-- ====================== 1.4 Iterative Denoising ====================== -->
<section class="section" id="p14-iterative">
  <h2>Part 1.4 — Iterative Denoising with a Strided Schedule</h2>
  <p>
    I implement a strided DDPM-style reverse process using the pretrained DeepFloyd Stage I UNet.
    Starting from a moderately noisy Campanile, I iteratively predict the clean image estimate,
    step toward a less noisy timestep, and add the model-predicted variance. I display every 5th
    step of the denoising loop and compare the final result against one-step denoising and a
    Gaussian blur baseline.
  </p>

  <div class="callout">
    <strong>Deliverables</strong>
    <ul>
      <li>Create <span class="kbd">strided_timesteps</span> from 990 to 0 in steps of 30 and initialize the scheduler.</li>
      <li>Implement <span class="kbd">iterative_denoise</span>.</li>
      <li>Show the noisy Campanile every 5th loop of denoising.</li>
      <li>Compare final iterative result vs. one-step vs. Gaussian blur.</li>
    </ul>
  </div>

  <div class="figure">
<pre class="codeblock"><code>strided_timesteps = list(range(990, -1, -30))

stage_1.scheduler.set_timesteps(timesteps=strided_timesteps)

import numpy as np
def iterative_denoise(im_noisy, i_start, prompt_embeds, timesteps, display=True):
  image = im_noisy.clone()
  prompt_embeds = prompt_embeds.to(device).half()
  with torch.no_grad():
    for i in range(i_start, len(timesteps) - 1):
      t = timesteps[i]
      prev_t = timesteps[i+1]

      alpha_cumprod_t = alphas_cumprod[t].to(device)
      alpha_cumprod_t_prev = alphas_cumprod[prev_t].to(device)
      alpha_t = alpha_cumprod_t / alpha_cumprod_t_prev
      beta_t = 1 - alpha_t

      model_output = stage_1.unet(
          image,
          torch.tensor(t).to(device),
          encoder_hidden_states=prompt_embeds,
          return_dict=False
      )[0]

      noise_est, predicted_variance = torch.split(model_output, image.shape[1], dim=1)

      x0_est = (image - torch.sqrt(1 - alpha_cumprod_t) * noise_est) / torch.sqrt(alpha_cumprod_t)
      x0_est = torch.clamp(x0_est, -1, 1)

      coeff1 = (torch.sqrt(alpha_cumprod_t_prev) * beta_t) / (1 - alpha_cumprod_t)
      coeff2 = (torch.sqrt(alpha_t) * (1 - alpha_cumprod_t_prev)) / (1 - alpha_cumprod_t)
      pred_prev_image = coeff1 * x0_est + coeff2 * image
      if i < len(timesteps) - 1:
        pred_prev_image = add_variance(predicted_variance, torch.tensor(t).to(device), pred_prev_image)
      image = pred_prev_image
      if display and i % 5 == 0:
        print(f"Iteration {i}, t={t} ->; {prev_t}")
        vis_img = image[0].permute(1, 2, 0).cpu().float() / 2.0 + 0.5
        media.show_image(vis_img, height=128)

  clean = image.cpu().detach().numpy()
  clean = (clean / 2.0 + 0.5).squeeze().transpose(1, 2, 0)
  clean = np.clip(clean, 0, 1)

  return clean

prompt_embeds = prompt_embeds_dict["a high quality photo"]

i_start = 10
t_start_val = strided_timesteps[i_start]
im_noisy = forward(test_im, t).half().to(device)

print(f"Starting Denoising from Step {i_start} (t={t_start_val})")
media.show_image(im_noisy[0].permute(1,2,0).cpu().float() / 2.0 + 0.5, title="Initial Noisy Image")

clean_iterative = iterative_denoise(im_noisy,
                          i_start=i_start,
                          prompt_embeds=prompt_embeds,
                          timesteps=strided_timesteps)

with torch.no_grad():
    alpha_cumprod_start = alphas_cumprod[t_start_val].to(device)

    noise_est_onestep = stage_1.unet(
        im_noisy,
        torch.tensor(t_start_val).to(device),
        encoder_hidden_states=prompt_embeds.to(device).half(),
        return_dict=False
    )[0][:, :3]

    clean_one_step = (im_noisy - torch.sqrt(1 - alpha_cumprod_start) * noise_est_onestep) / torch.sqrt(alpha_cumprod_start)
    clean_one_step = torch.clamp(clean_one_step, -1, 1)

    clean_one_step_vis = clean_one_step[0].permute(1, 2, 0).cpu().float() / 2.0 + 0.5

clean_gaussian_vis = TF.gaussian_blur(im_noisy, kernel_size=5)[0].permute(1, 2, 0).cpu().float() / 2.0 + 0.5
print("Final Results Comparison:")
media.show_images(
    [clean_iterative, clean_one_step_vis, clean_gaussian_vis],
    titles=["Iteratively Denoised", "One-Step Estimate", "Gaussian Blur"],
    height=256
)</code></pre>
    <figcaption class="figcap">
      Strided iterative denoising implementation and comparison code used for the Campanile experiment.
    </figcaption>
  </div>

  <h3>Denoising progression (every 5th step)</h3>
  <p>
    The sequence below shows the Campanile becoming progressively less noisy as the loop advances.
  </p>

  <div class="gallery figure-center">
    <figure class="figure">
      <img src="p5/it_10.png" alt="Iteration 10 denoising result"/>
      <figcaption class="figcap">Iteration <span class="kbd">10</span> (t = 690 → 660).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/it_15.png" alt="Iteration 15 denoising result"/>
      <figcaption class="figcap">Iteration <span class="kbd">15</span> (t = 540 → 510).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/it_20.png" alt="Iteration 20 denoising result"/>
      <figcaption class="figcap">Iteration <span class="kbd">20</span> (t = 390 → 360).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/it_25.png" alt="Iteration 25 denoising result"/>
      <figcaption class="figcap">Iteration <span class="kbd">25</span> (t = 240 → 210).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/it_30.png" alt="Iteration 30 denoising result"/>
      <figcaption class="figcap">Iteration <span class="kbd">30</span> (t = 90 → 60).</figcaption>
    </figure>
  </div>

  <h3>Final comparison</h3>
  <div class="gallery" style="grid-template-columns: repeat(3, minmax(0, 1fr));">
    <figure class="figure">
      <img src="p5/iter_final.png" alt="Final iteratively denoised Campanile"/>
      <figcaption class="figcap">Final iteratively denoised result.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/onestep_final.png" alt="Final one-step denoised Campanile"/>
      <figcaption class="figcap">One-step denoised estimate.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/gaussian_final.png" alt="Final Gaussian blurred Campanile"/>
      <figcaption class="figcap">Gaussian blur baseline.</figcaption>
    </figure>
  </div>

  <div class="answer">
    <div class="label">Observations</div>
    <ul>
      <li>The iterative method steadily recovers structure and color as noise decreases across the strided schedule.</li>
      <li>Compared to one-step denoising, the multi-step approach produces a cleaner, more coherent reconstruction.</li>
      <li>Gaussian blur reduces high-frequency noise but cannot restore lost structure, especially at higher noise levels.</li>
    </ul>
  </div>
</section>

    <!-- ====================== 1.5 Sampling from Noise ====================== -->
    <section class="section" id="p15-sampling">
      <h2>Part 1.5 — Diffusion Model Sampling</h2>
      <p>
        By setting <span class="kbd">i_start = 0</span> and initializing with pure Gaussian noise, I use the
        iterative denoising loop to generate images from scratch using the weak prompt
        <span class="kbd">"a high quality photo"</span>.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Five sampled images.</li>
        </ul>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5/sample_1.png" alt="Sample 1"/>
          <figcaption class="figcap">Sample 1.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/sample_2.png" alt="Sample 2"/>
          <figcaption class="figcap">Sample 2.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/sample_3.png" alt="Sample 3"/>
          <figcaption class="figcap">Sample 3.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/sample_4.png" alt="Sample 4"/>
          <figcaption class="figcap">Sample 4.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/sample_5.png" alt="Sample 5"/>
          <figcaption class="figcap">Sample 5.</figcaption>
        </figure>
      </div>
    </section>

    <!-- ====================== 1.6 CFG ====================== -->
    <section class="section" id="p16-cfg">
      <h2>Part 1.6 — Classifier-Free Guidance (CFG)</h2>
      <p>
        I implement <span class="kbd">iterative_denoise_cfg</span> by combining conditional and unconditional
        noise estimates. With a positive CFG scale, generation quality improves significantly at the cost of
        reduced diversity.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>CFG-enabled denoising loop implementation.</li>
          <li>Five samples of <span class="kbd">"a high quality photo"</span> using CFG.</li>
        </ul>
      </div>

      <div class="figure">
<pre class="codeblock"><code>def iterative_denoise_cfg(im_noisy, i_start, prompt_embeds, uncond_prompt_embeds, timesteps, scale=7, display=True):
  image = im_noisy.clone()
  prompt_embeds = prompt_embeds.to(device).half()
  uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()
  with torch.no_grad():
    for i in range(i_start, len(timesteps) - 1):
      t = timesteps[i]
      prev_t = timesteps[i+1]

      alpha_cumprod_t = alphas_cumprod[t].to(device)
      alpha_cumprod_t_prev = alphas_cumprod[prev_t].to(device)
      alpha_t = alpha_cumprod_t / alpha_cumprod_t_prev
      beta_t = 1 - alpha_t

      model_output = stage_1.unet(
          image,
          torch.tensor(t).to(device),
          encoder_hidden_states=prompt_embeds,
          return_dict=False
      )[0]

      uncond_model_output = stage_1.unet(
          image,
          torch.tensor(t).to(device),
          encoder_hidden_states=uncond_prompt_embeds,
          return_dict=False
      )[0]

      noise_est, predicted_variance = torch.split(model_output, image.shape[1], dim=1)
      uncond_noise_est, _ = torch.split(uncond_model_output, image.shape[1], dim=1)

      noise_est = uncond_noise_est + scale * (noise_est - uncond_noise_est)

      x0_est = (image - torch.sqrt(1 - alpha_cumprod_t) * noise_est) / torch.sqrt(alpha_cumprod_t)
      x0_est = torch.clamp(x0_est, -1, 1)

      coeff1 = (torch.sqrt(alpha_cumprod_t_prev) * beta_t) / (1 - alpha_cumprod_t)
      coeff2 = (torch.sqrt(alpha_t) * (1 - alpha_cumprod_t_prev)) / (1 - alpha_cumprod_t)

      pred_prev_image = coeff1 * x0_est + coeff2 * image

      if i < len(timesteps) - 1:
        pred_prev_image = add_variance(predicted_variance, torch.tensor(t).cpu(), pred_prev_image)
      image = pred_prev_image
      if display and i % 10 == 0:
        print(f"  Step {i} (t={t})")
        vis_img = image[0].permute(1, 2, 0).cpu().float() / 2.0 + 0.5
        media.show_image(vis_img, height=128)

  clean = image.cpu().detach().numpy()
  clean = (clean / 2.0 + 0.5).squeeze().transpose(1, 2, 0)
  clean = np.clip(clean, 0, 1)

  return clean</code></pre>
  <figcaption class="figcap">
    Classifier-Free Guidance (CFG) iterative denoising loop used to improve text-conditional sample quality.
  </figcaption>
</div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5/sample_cfg_1.png" alt="Sample 1 with CFG"/>
          <figcaption class="figcap">Sample 1 (CFG).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/sample_cfg_2.png" alt="Sample 2 with CFG"/>
          <figcaption class="figcap">Sample 2 (CFG).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/sample_cfg_3.png" alt="Sample 3 with CFG"/>
          <figcaption class="figcap">Sample 3 (CFG).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/sample_cfg_4.png" alt="Sample 4 with CFG"/>
          <figcaption class="figcap">Sample 4 (CFG).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/sample_cfg_5.png" alt="Sample 5 with CFG"/>
          <figcaption class="figcap">Sample 5 (CFG).</figcaption>
        </figure>
      </div>
    </section>

<!-- ====================== 1.7 Image-to-image Translation ====================== -->
<section class="section" id="p17-sdedit">
  <h2>Part 1.7 — Image-to-image Translation (SDEdit)</h2>
  <p>
    Using my forward process and the CFG denoising loop, I apply the SDEdit-style procedure:
    I add noise to an input image at a chosen starting index and then iteratively denoise it with the
    conditional prompt <span class="kbd">"a high quality photo"</span>. Smaller <span class="kbd">i_start</span>
    values correspond to heavier edits, while larger values preserve more of the original structure.
  </p>

  <div class="callout">
    <strong>Deliverables</strong>
    <ul>
      <li>Edits of the Campanile at <span class="kbd">i_start = [1, 3, 5, 7, 10, 20]</span>.</li>
      <li>Edits of 2 of my own images using the same procedure.</li>
    </ul>
  </div>

  <!-- ---------------------- Campanile ---------------------- -->
  <h3>Campanile</h3>
  <div class="figure figure-center" style="max-width:520px;">
    <img src="p5/campanile_clean.png" alt="Original Campanile"/>
    <figcaption class="figcap">Original Campanile.</figcaption>
  </div>

  <div class="gallery">
    <figure class="figure">
      <img src="p5/campanile_sdedit_1.png" alt="Campanile SDEdit i_start=1"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=1</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_sdedit_3.png" alt="Campanile SDEdit i_start=3"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=3</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_sdedit_5.png" alt="Campanile SDEdit i_start=5"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=5</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_sdedit_7.png" alt="Campanile SDEdit i_start=7"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=7</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_sdedit_10.png" alt="Campanile SDEdit i_start=10"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=10</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_sdedit_20.png" alt="Campanile SDEdit i_start=20"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=20</span>.</figcaption>
    </figure>
  </div>

  <!-- ---------------------- Personal Image 1 ---------------------- -->
  <h3>Personal image 1</h3>
  <div class="figure figure-center" style="max-width:520px;">
    <img src="p5/personal1_clean.png" alt="Original personal image 1"/>
    <figcaption class="figcap">Original personal image 1 (input image).</figcaption>
  </div>

  <div class="gallery">
    <figure class="figure">
      <img src="p5/personal1_sdedit_1.png" alt="Personal image 1 SDEdit i_start=1"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=1</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal1_sdedit_3.png" alt="Personal image 1 SDEdit i_start=3"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=3</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal1_sdedit_5.png" alt="Personal image 1 SDEdit i_start=5"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=5</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal1_sdedit_7.png" alt="Personal image 1 SDEdit i_start=7"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=7</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal1_sdedit_10.png" alt="Personal image 1 SDEdit i_start=10"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=10</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal1_sdedit_20.png" alt="Personal image 1 SDEdit i_start=20"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=20</span>.</figcaption>
    </figure>
  </div>

  <!-- ---------------------- Personal Image 2 ---------------------- -->
  <h3>Personal image 2</h3>
  <div class="figure figure-center" style="max-width:520px;">
    <img src="p5/personal2_clean.png" alt="Original personal image 2"/>
    <figcaption class="figcap">Original personal image 2 (input image).</figcaption>
  </div>

  <div class="gallery">
    <figure class="figure">
      <img src="p5/personal2_sdedit_1.png" alt="Personal image 2 SDEdit i_start=1"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=1</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal2_sdedit_3.png" alt="Personal image 2 SDEdit i_start=3"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=3</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal2_sdedit_5.png" alt="Personal image 2 SDEdit i_start=5"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=5</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal2_sdedit_7.png" alt="Personal image 2 SDEdit i_start=7"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=7</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal2_sdedit_10.png" alt="Personal image 2 SDEdit i_start=10"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=10</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal2_sdedit_20.png" alt="Personal image 2 SDEdit i_start=20"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=20</span>.</figcaption>
    </figure>
  </div>

  <div class="answer">
    <div class="label">Observations</div>
    <ul>
      <li>Lower <span class="kbd">i_start</span> values induce stronger edits because the model must “invent” more content while denoising.</li>
      <li>Higher <span class="kbd">i_start</span> values preserve the original structure more faithfully and look closer to the input.</li>
      <li>This behavior is consistent across the Campanile and my two personal images.</li>
    </ul>
  </div>
</section>


<!-- ====================== 1.7.1 Editing Hand-Drawn and Web Images ====================== -->
<section class="section" id="p171-web-handdrawn">
  <h2>Part 1.7.1 — Editing Hand-Drawn and Web Images</h2>
  <p>
    I apply the same SDEdit + CFG procedure to non-realistic inputs. Starting from a web image and two
    hand-drawn sketches, I add noise and iteratively denoise with the conditional prompt
    <span class="kbd">"a high quality photo"</span>. This tends to “project” the inputs onto the natural-image
    manifold, producing fun and sometimes surprising edits.
  </p>

  <div class="callout">
    <strong>Deliverables</strong>
    <ul>
      <li>1 web image edited at <span class="kbd">i_start = [1, 3, 5, 7, 10, 20]</span>.</li>
      <li>2 hand-drawn images edited at <span class="kbd">i_start = [1, 3, 5, 7, 10, 20]</span>.</li>
    </ul>
  </div>

  <!-- ---------------------- Web Image ---------------------- -->
  <h3>Web image</h3>
  <div class="figure figure-center" style="max-width:520px;">
    <img src="p5/web_clean.png" alt="Original web image"/>
    <figcaption class="figcap">Original web image (input).</figcaption>
  </div>

  <div class="gallery">
    <figure class="figure">
      <img src="p5/web_sdedit_1.png" alt="Web image SDEdit i_start=1"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=1</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/web_sdedit_3.png" alt="Web image SDEdit i_start=3"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=3</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/web_sdedit_5.png" alt="Web image SDEdit i_start=5"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=5</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/web_sdedit_7.png" alt="Web image SDEdit i_start=7"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=7</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/web_sdedit_10.png" alt="Web image SDEdit i_start=10"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=10</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/web_sdedit_20.png" alt="Web image SDEdit i_start=20"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=20</span>.</figcaption>
    </figure>
  </div>

  <!-- ---------------------- Hand-Drawn Image 1 ---------------------- -->
  <h3>Hand-drawn image 1</h3>
  <div class="figure figure-center" style="max-width:520px;">
    <img src="p5/sketch1_clean.png" alt="Original hand-drawn image 1"/>
    <figcaption class="figcap">Original hand-drawn image 1 (input).</figcaption>
  </div>

  <div class="gallery">
    <figure class="figure">
      <img src="p5/sketch1_sdedit_1.png" alt="Hand-drawn 1 SDEdit i_start=1"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=1</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/sketch1_sdedit_3.png" alt="Hand-drawn 1 SDEdit i_start=3"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=3</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/sketch1_sdedit_5.png" alt="Hand-drawn 1 SDEdit i_start=5"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=5</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/sketch1_sdedit_7.png" alt="Hand-drawn 1 SDEdit i_start=7"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=7</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/sketch1_sdedit_10.png" alt="Hand-drawn 1 SDEdit i_start=10"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=10</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/sketch1_sdedit_20.png" alt="Hand-drawn 1 SDEdit i_start=20"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=20</span>.</figcaption>
    </figure>
  </div>

  <!-- ---------------------- Hand-Drawn Image 2 ---------------------- -->
  <h3>Hand-drawn image 2</h3>
  <div class="figure figure-center" style="max-width:520px;">
    <img src="p5/sketch2_clean.png" alt="Original hand-drawn image 2"/>
    <figcaption class="figcap">Original hand-drawn image 2 (input).</figcaption>
  </div>

  <div class="gallery">
    <figure class="figure">
      <img src="p5/sketch2_sdedit_1.png" alt="Hand-drawn 2 SDEdit i_start=1"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=1</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/sketch2_sdedit_3.png" alt="Hand-drawn 2 SDEdit i_start=3"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=3</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/sketch2_sdedit_5.png" alt="Hand-drawn 2 SDEdit i_start=5"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=5</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/sketch2_sdedit_7.png" alt="Hand-drawn 2 SDEdit i_start=7"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=7</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/sketch2_sdedit_10.png" alt="Hand-drawn 2 SDEdit i_start=10"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=10</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/sketch2_sdedit_20.png" alt="Hand-drawn 2 SDEdit i_start=20"/>
      <figcaption class="figcap">SDEdit with <span class="kbd">i_start=20</span>.</figcaption>
    </figure>
  </div>

  <div class="answer">
    <div class="label">Observations</div>
    <ul>
      <li>Compared to realistic photos, these inputs are transformed more aggressively into plausible “photo-like” textures.</li>
      <li>The effect is strongest at low <span class="kbd">i_start</span>, where the model must reconstruct more from noise.</li>
    </ul>
  </div>
</section>

<!-- ====================== 1.7.2 Inpainting ====================== -->
<section class="section" id="p172-inpainting">
  <h2>Part 1.7.2 — Inpainting (RePaint-style)</h2>
  <p>
    I implement inpainting by modifying the CFG denoising loop. At each step, after predicting the next
    less-noisy image, I overwrite pixels <em>outside</em> the edit region with the appropriately noised
    original image. This preserves the unmasked content while letting the model synthesize new content
    inside the mask.
  </p>

  <div class="callout">
    <strong>Deliverables</strong>
    <ul>
      <li>A correctly implemented <span class="kbd">inpaint</span> function.</li>
      <li>Campanile inpainting (with my mask).</li>
      <li>Two additional inpainted examples on my own images.</li>
    </ul>
  </div>

  <div class="figure">
<pre class="codeblock"><code>def inpaint(original_image, mask, prompt_embeds, uncond_prompt_embeds, timesteps, scale=7, display=True):
    image = torch.randn_like(original_image).to(device).half()

    original_image = original_image.to(device).half()
    mask = mask.to(device).half()
    prompt_embeds = prompt_embeds.to(device).half()
    uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

    with torch.no_grad():
        for i in range(0, len(timesteps) - 1):
            t = timesteps[i]
            prev_t = timesteps[i+1]

            alpha_cumprod_t = alphas_cumprod[t].to(device)
            alpha_cumprod_t_prev = alphas_cumprod[prev_t].to(device)
            alpha_t = alpha_cumprod_t / alpha_cumprod_t_prev
            beta_t = 1 - alpha_t

            model_output = stage_1.unet(
                image, torch.tensor(t).to(device),
                encoder_hidden_states=prompt_embeds, return_dict=False)[0]

            uncond_model_output = stage_1.unet(
                image, torch.tensor(t).to(device),
                encoder_hidden_states=uncond_prompt_embeds, return_dict=False)[0]

            noise_est, predicted_variance = torch.split(model_output, image.shape[1], dim=1)
            uncond_noise_est, _ = torch.split(uncond_model_output, image.shape[1], dim=1)
            noise_est = uncond_noise_est + scale * (noise_est - uncond_noise_est)

            x0_est = (image - torch.sqrt(1 - alpha_cumprod_t) * noise_est) / torch.sqrt(alpha_cumprod_t)
            x0_est = torch.clamp(x0_est, -1, 1)

            coeff1 = (torch.sqrt(alpha_cumprod_t_prev) * beta_t) / (1 - alpha_cumprod_t)
            coeff2 = (torch.sqrt(alpha_t) * (1 - alpha_cumprod_t_prev)) / (1 - alpha_cumprod_t)
            pred_prev_image = coeff1 * x0_est + coeff2 * image

            if i < len(timesteps) - 1:
                pred_prev_image = add_variance(predicted_variance, torch.tensor(t).cpu(), pred_prev_image)

            noisy_original = forward(original_image, prev_t)
            image = mask * pred_prev_image + (1 - mask) * noisy_original

            if display and i % 10 == 0:
                print(f"Step {i} (t={t})")
                vis_img = image[0].permute(1, 2, 0).cpu().float() / 2.0 + 0.5
                media.show_image(vis_img, height=128)

    clean = image.cpu().detach().numpy()
    clean = (clean / 2.0 + 0.5).squeeze().transpose(1, 2, 0)
    clean = np.clip(clean, 0, 1)
    return clean</code></pre>
    <figcaption class="figcap">
      Inpainting loop with CFG and a RePaint-style overwrite step using the forward process.
    </figcaption>
  </div>

  <h3>Campanile inpainting</h3>
  <div class="grid">
    <figure class="figure">
      <img src="p5/campanile_clean.png" alt="Campanile original for inpainting"/>
      <figcaption class="figcap">Original image.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_mask.png" alt="Campanile inpainting mask"/>
      <figcaption class="figcap">Mask.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_inpainted.png" alt="Campanile inpainted result"/>
      <figcaption class="figcap">Inpainted result.</figcaption>
    </figure>
  </div>

  <h3>Personal image 1 inpainting</h3>
  <div class="grid">
    <figure class="figure">
      <img src="p5/personal1_inpaint_original.jpg" alt="Personal image 1 original for inpainting"/>
      <figcaption class="figcap">Original image.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal1_inpaint_mask.png" alt="Personal image 1 inpainting mask"/>
      <figcaption class="figcap">Mask.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal1_inpaint_result.png" alt="Personal image 1 inpainted result"/>
      <figcaption class="figcap">Inpainted result.</figcaption>
    </figure>
  </div>

  <h3>Personal image 2 inpainting</h3>
  <div class="grid">
    <figure class="figure">
      <img src="p5/personal2_inpaint_original.jpg" alt="Personal image 2 original for inpainting"/>
      <figcaption class="figcap">Original image.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal2_inpaint_mask.png" alt="Personal image 2 inpainting mask"/>
      <figcaption class="figcap">Mask.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal2_inpaint_result.png" alt="Personal image 2 inpainted result"/>
      <figcaption class="figcap">Inpainted result.</figcaption>
    </figure>
  </div>
</section>


<!-- ====================== 1.7.3 Text-Conditional Image-to-image Translation ====================== -->
<section class="section" id="p173-text-conditional">
  <h2>Part 1.7.3 — Text-Conditional Image-to-image Translation (CFG)</h2>
  <p>
    I repeat the SDEdit-style pipeline, but now condition the denoising with my own text prompts.
    As I increase the noise level (lower <span class="kbd">i_start</span>), the model has more freedom
    to reinterpret the image to match the prompt; with higher <span class="kbd">i_start</span>, the output
    stays closer to the original.
  </p>

  <div class="callout">
    <strong>Deliverables</strong>
    <ul>
      <li>Edits of the Campanile at <span class="kbd">i_start = [1, 3, 5, 7, 10, 20]</span> using a custom prompt.</li>
      <li>Edits of two of my own images with the same procedure and prompt.</li>
    </ul>
  </div>

  <!-- ---------------------- Campanile ---------------------- -->
  <h3>Campanile (text-conditional)</h3>
  <p class="proj-sub" style="margin-top:-6px;">
    Prompt: <span class="kbd">a lithograph of waterfalls</span>
  </p>

  <div class="figure figure-center" style="max-width:520px;">
    <img src="p5/campanile_clean.png" alt="Original Campanile for text-conditional editing"/>
    <figcaption class="figcap">Original Campanile (input).</figcaption>
  </div>

  <div class="gallery">
    <figure class="figure">
      <img src="p5/campanile_text_1.png" alt="Campanile text-conditional edit i_start=1"/>
      <figcaption class="figcap">Prompted edit with <span class="kbd">i_start=1</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_text_3.png" alt="Campanile text-conditional edit i_start=3"/>
      <figcaption class="figcap">Prompted edit with <span class="kbd">i_start=3</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_text_5.png" alt="Campanile text-conditional edit i_start=5"/>
      <figcaption class="figcap">Prompted edit with <span class="kbd">i_start=5</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_text_7.png" alt="Campanile text-conditional edit i_start=7"/>
      <figcaption class="figcap">Prompted edit with <span class="kbd">i_start=7</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_text_10.png" alt="Campanile text-conditional edit i_start=10"/>
      <figcaption class="figcap">Prompted edit with <span class="kbd">i_start=10</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_text_20.png" alt="Campanile text-conditional edit i_start=20"/>
      <figcaption class="figcap">Prompted edit with <span class="kbd">i_start=20</span>.</figcaption>
    </figure>
  </div>

  <!-- ---------------------- Personal Image 1 ---------------------- -->
  <h3>Personal image 1 (text-conditional)</h3>
  <p class="proj-sub" style="margin-top:-6px;">
    Prompt: <span class="kbd">a photo of a dog</span>
  </p>

  <div class="figure figure-center" style="max-width:520px;">
    <img src="p5/personal1_inpaint_original.jpg" alt="Original personal image 1 for text-conditional editing"/>
    <figcaption class="figcap">Original personal image 1 (input).</figcaption>
  </div>

  <div class="gallery">
    <figure class="figure">
      <img src="p5/personal1_text_1.jpg" alt="Personal image 1 text-conditional edit i_start=1"/>
      <figcaption class="figcap"><span class="kbd">i_start=1</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal1_text_3.jpg" alt="Personal image 1 text-conditional edit i_start=3"/>
      <figcaption class="figcap"><span class="kbd">i_start=3</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal1_text_5.jpg" alt="Personal image 1 text-conditional edit i_start=5"/>
      <figcaption class="figcap"><span class="kbd">i_start=5</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal1_text_7.jpg" alt="Personal image 1 text-conditional edit i_start=7"/>
      <figcaption class="figcap"><span class="kbd">i_start=7</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal1_text_10.jpg" alt="Personal image 1 text-conditional edit i_start=10"/>
      <figcaption class="figcap"><span class="kbd">i_start=10</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal1_text_20.jpg" alt="Personal image 1 text-conditional edit i_start=20"/>
      <figcaption class="figcap"><span class="kbd">i_start=20</span>.</figcaption>
    </figure>
  </div>

  <!-- ---------------------- Personal Image 2 ---------------------- -->
  <h3>Personal image 2 (text-conditional)</h3>
  <p class="proj-sub" style="margin-top:-6px;">
    Prompt: <span class="kbd">a photo of a snowy mountain</span>
  </p>

  <div class="figure figure-center" style="max-width:520px;">
    <img src="p5/personal2_inpaint_original.jpg" alt="Original personal image 2 for text-conditional editing"/>
    <figcaption class="figcap">Original personal image 2 (input).</figcaption>
  </div>

  <div class="gallery">
    <figure class="figure">
      <img src="p5/personal2_text_1.png" alt="Personal image 2 text-conditional edit i_start=1"/>
      <figcaption class="figcap"><span class="kbd">i_start=1</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal2_text_3.png" alt="Personal image 2 text-conditional edit i_start=3"/>
      <figcaption class="figcap"><span class="kbd">i_start=3</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal2_text_5.png" alt="Personal image 2 text-conditional edit i_start=5"/>
      <figcaption class="figcap"><span class="kbd">i_start=5</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal2_text_7.png" alt="Personal image 2 text-conditional edit i_start=7"/>
      <figcaption class="figcap"><span class="kbd">i_start=7</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal2_text_10.png" alt="Personal image 2 text-conditional edit i_start=10"/>
      <figcaption class="figcap"><span class="kbd">i_start=10</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/personal2_text_20.png" alt="Personal image 2 text-conditional edit i_start=20"/>
      <figcaption class="figcap"><span class="kbd">i_start=20</span>.</figcaption>
    </figure>
  </div>
</section>



<!-- ====================== 1.8 Visual Anagrams (Flip Illusions) ====================== -->
<section class="section" id="p18-anagrams">
  <h2>Part 1.8 — Visual Anagrams</h2>
  <p>
    In this part I implement a flip-based visual anagram pipeline using DeepFloyd Stage I.
    The idea is to denoise an image with one prompt in the upright orientation, denoise the
    vertically flipped image with a second prompt, flip the second noise estimate back, and
    average the two noise predictions before taking the next DDPM step. This creates an image
    that can plausibly satisfy two different interpretations when viewed upright vs. upside down.
  </p>

  <div class="callout">
    <strong>Deliverables</strong>
    <ul>
      <li>Correctly implemented flip-illusion function.</li>
      <li>Two illusions that change appearance when flipped upside down.</li>
    </ul>
  </div>

  <div class="figure">
<pre class="codeblock"><code>def make_flip_illusion(image, i_start, prompt_embeds_1, prompt_embeds_2, uncond_prompt_embeds, timesteps, scale=7, display=True):
    image = image.to(device).half()
    prompt_embeds_1 = prompt_embeds_1.to(device).half()
    prompt_embeds_2 = prompt_embeds_2.to(device).half()
    uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

    with torch.no_grad():
        for i in range(i_start, len(timesteps) - 1):
            t = timesteps[i]
            prev_t = timesteps[i+1]

            alpha_cumprod_t = alphas_cumprod[t].to(device)
            alpha_cumprod_t_prev = alphas_cumprod[prev_t].to(device)
            alpha_t = alpha_cumprod_t / alpha_cumprod_t_prev
            beta_t = 1 - alpha_t

            model_out_1 = stage_1.unet(
                image,
                torch.tensor(t).to(device),
                encoder_hidden_states=prompt_embeds_1,
                return_dict=False
            )[0]
            uncond_out_1 = stage_1.unet(
                image,
                torch.tensor(t).to(device),
                encoder_hidden_states=uncond_prompt_embeds,
                return_dict=False
            )[0]

            noise_1, var_1 = torch.split(model_out_1, image.shape[1], dim=1)
            uncond_noise_1, _ = torch.split(uncond_out_1, image.shape[1], dim=1)
            noise_pred_1 = uncond_noise_1 + scale * (noise_1 - uncond_noise_1)

            image_flipped = torch.flip(image, [2])

            model_out_2 = stage_1.unet(
                image_flipped,
                torch.tensor(t).to(device),
                encoder_hidden_states=prompt_embeds_2,
                return_dict=False
            )[0]
            uncond_out_2 = stage_1.unet(
                image_flipped,
                torch.tensor(t).to(device),
                encoder_hidden_states=uncond_prompt_embeds,
                return_dict=False
            )[0]

            noise_2, var_2 = torch.split(model_out_2, image.shape[1], dim=1)
            uncond_noise_2, _ = torch.split(uncond_out_2, image.shape[1], dim=1)
            noise_pred_2 = uncond_noise_2 + scale * (noise_2 - uncond_noise_2)

            noise_pred_2 = torch.flip(noise_pred_2, [2])
            avg_variance = (var_1 + torch.flip(var_2, [2])) / 2.0

            final_noise = (noise_pred_1 + noise_pred_2) / 2.0

            x0_est = (image - torch.sqrt(1 - alpha_cumprod_t) * final_noise) / torch.sqrt(alpha_cumprod_t)
            x0_est = torch.clamp(x0_est, -1, 1)

            coeff1 = (torch.sqrt(alpha_cumprod_t_prev) * beta_t) / (1 - alpha_cumprod_t)
            coeff2 = (torch.sqrt(alpha_t) * (1 - alpha_cumprod_t_prev)) / (1 - alpha_cumprod_t)
            pred_prev_image = coeff1 * x0_est + coeff2 * image

            if i < len(timesteps) - 1:
                pred_prev_image = add_variance(avg_variance, torch.tensor(t).cpu(), pred_prev_image)

            image = pred_prev_image

            if display and i % 10 == 0:
                pass

    clean = image.cpu().detach().numpy()
    clean = (clean / 2.0 + 0.5).squeeze().transpose(1, 2, 0)
    clean = np.clip(clean, 0, 1)
    return clean</code></pre>
    <figcaption class="figcap">
      Flip-illusion sampling loop used to create visual anagrams by averaging upright and flipped CFG noise estimates.
    </figcaption>
  </div>

  <h3>Illusion 1</h3>
  <p class="proj-sub">
    Prompt pair: <span class="kbd">[an oil painting of an old man]</span> (upright) · <span class="kbd">[an oil painting of people around a campfire]</span> (flipped)
  </p>
  <div class="gallery">
    <figure class="figure">
      <img src="p5/anagram1_upright.png" alt="Flip illusion 1 upright"/>
      <figcaption class="figcap">Upright view (an oil painting of an old man).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/anagram1_flipped.png" alt="Flip illusion 1 flipped"/>
      <figcaption class="figcap">Flipped view (an oil painting of people around a campfire).</figcaption>
    </figure>
  </div>

  <h3>Illusion 2</h3>
  <p class="proj-sub">
    Prompt pair: <span class="kbd">[a photo of a snowy mountain]</span> (upright) · <span class="kbd">[an oil painting of an old man]</span> (flipped)
  </p>
  <div class="gallery">
    <figure class="figure">
      <img src="p5/anagram2_upright.png" alt="Flip illusion 2 upright"/>
      <figcaption class="figcap">Upright view (a photo of a snowy mountain).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/anagram2_flipped.png" alt="Flip illusion 2 flipped"/>
      <figcaption class="figcap">Flipped view (an oil painting of an old man).</figcaption>
    </figure>
  </div>
</section>


<!-- ====================== 1.9 Hybrid Images ====================== -->
<section class="section" id="p19-hybrids">
  <h2>Part 1.9 — Hybrid Images</h2>
  <p>
    In this part I implement a factorized diffusion approach to create hybrid images with DeepFloyd Stage I.
    I estimate two CFG noise predictions from two different prompts, then compose a combined noise estimate by
    taking low frequencies from the first and high frequencies from the second. This mirrors the classic hybrid
    image idea from Project 2, but performed inside the diffusion denoising loop. I use the recommended
    Gaussian blur settings (kernel size 33, sigma 2) for the low-pass filter.
  </p>

  <div class="callout">
    <strong>Deliverables</strong>
    <ul>
      <li>Correctly implemented <span class="kbd">make_hybrids</span> function.</li>
      <li>Two hybrid images of my choosing.</li>
    </ul>
  </div>

  <div class="figure">
<pre class="codeblock"><code>def make_hybrids(image, i_start, prompt_embeds_1, prompt_embeds_2, uncond_prompt_embeds, timesteps, scale=7, display=True):
    image = image.to(device).half()
    prompt_embeds_1 = prompt_embeds_1.to(device).half()
    prompt_embeds_2 = prompt_embeds_2.to(device).half()
    uncond_prompt_embeds = uncond_prompt_embeds.to(device).half()

    with torch.no_grad():
        for i in range(i_start, len(timesteps) - 1):
            t = timesteps[i]
            prev_t = timesteps[i+1]

            alpha_cumprod_t = alphas_cumprod[t].to(device)
            alpha_cumprod_t_prev = alphas_cumprod[prev_t].to(device)
            alpha_t = alpha_cumprod_t / alpha_cumprod_t_prev
            beta_t = 1 - alpha_t

            out_1 = stage_1.unet(image, torch.tensor(t).to(device), encoder_hidden_states=prompt_embeds_1, return_dict=False)[0]
            out_u = stage_1.unet(image, torch.tensor(t).to(device), encoder_hidden_states=uncond_prompt_embeds, return_dict=False)[0]

            noise_1, var_1 = torch.split(out_1, image.shape[1], dim=1)
            noise_u, _ = torch.split(out_u, image.shape[1], dim=1)
            eps_1 = noise_u + scale * (noise_1 - noise_u)

            out_2 = stage_1.unet(image, torch.tensor(t).to(device), encoder_hidden_states=prompt_embeds_2, return_dict=False)[0]

            noise_2, var_2 = torch.split(out_2, image.shape[1], dim=1)
            eps_2 = noise_u + scale * (noise_2 - noise_u)

            low_pass_1 = TF.gaussian_blur(eps_1, kernel_size=33, sigma=2)
            low_pass_2 = TF.gaussian_blur(eps_2, kernel_size=33, sigma=2)

            eps_combined = low_pass_1 + (eps_2 - low_pass_2)

            avg_variance = (var_1 + var_2) / 2.0

            x0_est = (image - torch.sqrt(1 - alpha_cumprod_t) * eps_combined) / torch.sqrt(alpha_cumprod_t)
            x0_est = torch.clamp(x0_est, -1, 1)

            coeff1 = (torch.sqrt(alpha_cumprod_t_prev) * beta_t) / (1 - alpha_cumprod_t)
            coeff2 = (torch.sqrt(alpha_t) * (1 - alpha_cumprod_t_prev)) / (1 - alpha_cumprod_t)
            pred_prev_image = coeff1 * x0_est + coeff2 * image

            if i < len(timesteps) - 1:
                pred_prev_image = add_variance(avg_variance, torch.tensor(t).cpu(), pred_prev_image)

            image = pred_prev_image

    clean = image.cpu().detach().numpy()
    clean = clean.astype(np.float32)
    clean = (clean / 2.0 + 0.5).squeeze().transpose(1, 2, 0)
    clean = np.clip(clean, 0, 1)
    return clean</code></pre>
    <figcaption class="figcap">
      Hybrid-image diffusion loop using low-pass noise from prompt 1 and high-pass noise from prompt 2.
    </figcaption>
  </div>

  <h3>Hybrid 1</h3>
  <p class="proj-sub">
    Prompt pair: <span class="kbd">[a lithograph of a skull — low frequencies]</span> ·
    <span class="kbd">[a lithograph of waterfalls — high frequencies]</span>
  </p>
  <div class="gallery">
    <figure class="figure">
      <img src="p5/hybrid1.png" alt="Hybrid image 1"/>
      <figcaption class="figcap">
        Final hybrid image. From far away the low-frequency prompt should dominate; up close the high-frequency prompt emerges.
      </figcaption>
    </figure>
  </div>

  <h3>Hybrid 2</h3>
  <p class="proj-sub">
    Prompt pair: <span class="kbd">[a photo of a galaxy — low frequencies]</span> ·
    <span class="kbd">[a photo of a human eye — high frequencies]</span>
  </p>
  <div class="gallery">
    <figure class="figure">
      <img src="p5/hybrid2.png" alt="Hybrid image 2"/>
      <figcaption class="figcap">Final hybrid image. From far away the low-frequency prompt should dominate; up close the high-frequency prompt emerges.</figcaption>
    </figure>
  </div>
</section>


    <!-- ====================== PART B (ADD ABOVE PART A) ====================== -->
    <section class="section" id="pb-overview">
      <h1 class="proj-title">Part B — Flow Matching on MNIST</h1>
      <p class="proj-sub">
        In Part B, I train a UNet on MNIST for (1) single-step denoising and
        (2) iterative generation using flow matching. I first validate the architecture
        on an L2 denoising objective, then add time-conditioning to predict the flow field
        from noise to data. I extend the model with class-conditioning and
        classifier-free guidance for faster convergence and controllable sampling.
      </p>

      <div class="callout">
        <strong>High-level structure</strong>
        <ul>
          <li><strong>Part B.1:</strong> Single-step denoising UNet on MNIST.</li>
          <li><strong>Part B.2:</strong> Time-conditioned flow matching UNet + iterative sampling.</li>
          <li><strong>Part B.2.4–2.6:</strong> Class-conditioning + CFG + scheduler ablation.</li>
        </ul>
      </div>

      <div class="callout">
        <strong>Jump to</strong>
        <div class="cta-row" style="margin-top:8px">
          <a class="btn secondary" href="#pb11-unet">B.1.1</a>
          <a class="btn secondary" href="#pb12-noise">B.1.2</a>
          <a class="btn secondary" href="#pb121-train">B.1.2.1</a>
          <a class="btn secondary" href="#pb122-ood">B.1.2.2</a>
          <a class="btn secondary" href="#pb123-purenoise">B.1.2.3</a>
          <a class="btn secondary" href="#pb21-time">B.2.1</a>
          <a class="btn secondary" href="#pb22-train">B.2.2</a>
          <a class="btn secondary" href="#pb23-sample">B.2.3</a>
          <a class="btn secondary" href="#pb24-class">B.2.4+</a>
        </div>
      </div>
    </section>

    <!-- ====================== B.1 Single-Step Denoising ====================== -->
    <section class="section" id="pb1-single-step">
      <h1 class="proj-title">Part B.1 — Training a Single-Step Denoising UNet</h1>
      <p>
        As a warmup, I train an unconditional UNet to map a noisy MNIST digit
        <span class="kbd">x_t</span> to the clean digit <span class="kbd">x</span>
        using an L2 denoising objective. This validates my UNet implementation
        before moving to flow matching.
      </p>
    </section>

    <!-- ====================== B.1.1 UNet Architecture ====================== -->
    <section class="section" id="pb11-unet">
      <h2>Part B.1.1 — Implementing the UNet</h2>
      <p>
        I implement the unconditional UNet using downsampling and upsampling blocks with
        skip connections following the provided architecture. I set the hidden dimension
        to <span class="kbd">D = 128</span> for the single-step denoiser.
      </p>

      <div class="answer">
        <div class="label">Implementation notes</div>
        <ul>
          <li>Down/Up blocks preserve spatial alignment for skip concatenations.</li>
          <li>BatchNorm + GELU used as specified in the diagram.</li>
          <li>All tensor shapes verified across the 28×28 → 7×7 → 28×28 path.</li>
        </ul>
      </div>
    </section>

  <!-- ====================== B 1.2 Noising Process Visualization ====================== -->
<section class="section" id="pb12-noising">
  <h2>Part B 1.2 — Visualizing the Noising Process</h2>
  <p>
    To understand the corruption process used for training the one-step denoiser, I visualize
    the effect of increasing noise level <span class="kbd">σ</span> on the same normalized MNIST digit.
    As <span class="kbd">σ</span> increases, the digit becomes progressively less recognizable.
  </p>

  <div class="callout">
    <strong>Deliverable</strong>
    <ul>
      <li>A visualization of the noising process using <span class="kbd">σ</span>.</li>
    </ul>
  </div>

  <figure class="figure figure-center">
    <img src="p5/noising_vis.png" alt="MNIST noising process across multiple sigma values"/>
    <figcaption class="figcap">
      Single-panel visualization of the noising process for increasing
      <span class="kbd">σ</span>, shown on the same example digit.
    </figcaption>
  </figure>
</section>

    <!-- ====================== B.1.2.1 Training ====================== -->
    <section class="section" id="pb121-train">
      <h2>Part B.1.2.1 — Training the One-Step Denoiser</h2>
      <p>
        I train the unconditional UNet on the MNIST training set for
        <span class="kbd">5</span> epochs using Adam with learning rate
        <span class="kbd">1e-4</span> and batch size <span class="kbd">256</span>.
        Noise is applied on-the-fly each batch to improve generalization.
      </p>

      <div class="callout">
        <strong>Recommended setup (used)</strong>
        <ul>
          <li>Model: UNet with <span class="kbd">D = 128</span></li>
          <li>Optimizer: Adam, <span class="kbd">lr = 1e-4</span></li>
          <li>Batch size: <span class="kbd">256</span></li>
          <li>Epochs: <span class="kbd">5</span></li>
        </ul>
      </div>

      <h3>Training loss</h3>
      <figure class="figure">
        <img src="p5/loss_curve.png" alt="Part B 1.2.1 training loss curve"/>
        <figcaption class="figcap">
          Training loss curve for the one-step denoising UNet.
        </figcaption>
      </figure>

      <h3>Test results with σ = 0.5</h3>
      <div class="gallery">
        <figure class="figure">
          <img src="p5/epoch1_sigma05.png" alt="One-step denoiser results after epoch 1"/>
          <figcaption class="figcap">After epoch <span class="kbd">1</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/epoch5_sigma05.png" alt="One-step denoiser results after epoch 5"/>
          <figcaption class="figcap">After epoch <span class="kbd">5</span>.</figcaption>
        </figure>
      </div>
    </section>

<!-- ====================== B 1.2.2 Out-of-Distribution Testing ====================== -->
<section class="section" id="pb122-ood">
  <h2>Part B 1.2.2 — Out-of-Distribution Noise Levels</h2>
  <p>
    After training the one-step denoising UNet on MNIST images noised with the
    in-distribution setting, I evaluate how well it generalizes to
    <em>out-of-distribution</em> noise levels. I keep the same test image and vary
    <span class="kbd">σ</span> to examine robustness as the corruption strength changes.
  </p>

  <div class="callout">
    <strong>Deliverable</strong>
    <ul>
      <li>
        Sample results on the test set with out-of-distribution noise levels after training,
        keeping the same image and varying <span class="kbd">σ</span>.
      </li>
    </ul>
  </div>

  <figure class="figure figure-center">
    <img src="p5/ood_vis.png" alt="OOD denoising results across multiple sigma values"/>
    <figcaption class="figcap">
      Single-panel comparison of denoising performance on the same test digit under
      varying <span class="kbd">σ</span> values (OOD relative to training).
    </figcaption>
  </figure>
</section>


    <!-- ====================== B.1.2.3 Denoising Pure Noise ====================== -->
    <section class="section" id="pb123-purenoise">
      <h2>Part B.1.2.3 — Denoising Pure Noise</h2>
      <p>
        To test whether one-step denoising can be used generatively, I repeat training
        but provide pure Gaussian noise as input and supervise toward clean MNIST targets.
        I visualize the outputs after epoch 1 and 5 and analyze the patterns the model tends to produce.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Training loss curve for the “pure-noise → digit” objective.</li>
          <li>Samples after epoch 1 and 5.</li>
          <li>Brief explanation of observed patterns.</li>
        </ul>
      </div>

      <h3>Training loss</h3>
      <figure class="figure">
        <img src="p5/loss_curve_denoise.png" alt="Pure-noise denoiser loss curve"/>
        <figcaption class="figcap">
          Training loss curve for the one-step model trained to denoise pure noise.
        </figcaption>
      </figure>

      <h3>Generated outputs</h3>
      <div class="gallery">
        <figure class="figure">
          <img src="p5/epoch1_samples.png" alt="Pure-noise denoiser samples after epoch 1"/>
          <figcaption class="figcap">After epoch <span class="kbd">1</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/epoch5_samples.png" alt="Pure-noise denoiser samples after epoch 5"/>
          <figcaption class="figcap">After epoch <span class="kbd">5</span>.</figcaption>
        </figure>
      </div>

<div class="answer">
  <div class="label">Brief reflection</div>

  <p>
    When I trained the one-step UNet to map <em>pure Gaussian noise</em> to clean MNIST digits using an
    <span class="kbd">L2 / MSE</span> objective, the model quickly converged to a highly consistent but
    uninformative output.
  </p>

  <ul>
    <li>
      <strong>The “ghostly” average:</strong>
      The generated images do not resemble sharp, specific digits (e.g., a clear “7” or “2”).
      Instead, they look like a blurry, static “ghost” digit, resembling a soft superposition of
      common MNIST strokes (often reminiscent of 0/1/8-like structure).
    </li>
    <li>
      <strong>Lack of diversity:</strong>
      Even with multiple distinct random noise inputs, the outputs are nearly identical. This suggests
      the network is largely ignoring the input variations.
    </li>
    <li>
      <strong>Loss plateau:</strong>
      The loss drops quickly early in training and then stabilizes at a non-zero value, unlike the
      in-distribution denoising task where the model can keep improving toward sharper reconstructions.
    </li>
  </ul>

  <p>
    This behavior is expected from the combination of <span class="kbd">MSE</span> loss and the absence of any
    meaningful correlation between input noise and target digit. Since a random noise vector contains
    no information about which digit it should map to, the optimal prediction under MSE becomes the
    conditional expectation of the target distribution. With no informative conditioning signal, this
    effectively collapses to the <em>global average</em> of the training images.
  </p>

  <p>
    In other words, the model learns to output the dataset “centroid” — the average MNIST digit — which
    explains both the ghostly appearance and the collapse in diversity.
  </p>
</div>

    </section>

    <!-- ====================== B.2 Flow Matching ====================== -->
    <section class="section" id="pb2-flow">
      <h1 class="proj-title">Part B.2 — Training a Flow Matching Model</h1>
      <p>
        One-step denoising is not sufficient for high-quality generation. I therefore train a
        time-conditioned UNet to predict the flow field that transports samples from noise to data.
        This enables iterative sampling from pure noise to recognizable MNIST digits.
      </p>
    </section>

    <!-- ====================== B.2.1 Time Conditioning ====================== -->
    <section class="section" id="pb21-time">
      <h2>Part B.2.1 — Adding Time Conditioning to UNet</h2>
      <p>
        I inject the scalar timestep <span class="kbd">t</span> into the UNet using two FCBlocks,
        modulating the <span class="kbd">unflatten</span> and <span class="kbd">up1</span> activations
        as specified. I normalize <span class="kbd">t</span> to <span class="kbd">[0, 1]</span>
        before embedding.
      </p>

<div class="answer">
  <div class="label">Implementation note (B 2.1 — Time conditioning)</div>
  <p>
    I added <strong>time conditioning</strong> to the UNet by introducing two FCBlocks that embed the scalar
    timestep <span class="kbd">t</span> and modulate intermediate feature maps at the specific points
    highlighted in the spec.
  </p>
  <ul>
    <li>
      <strong>Normalize time:</strong> I normalized the sampled timestep to
      <span class="kbd">t_norm = t / 999</span> so that the conditioning signal lies in
      <span class="kbd">[0, 1]</span>.
    </li>
    <li>
      <strong>Shape handling:</strong> I reshaped the time tensor to
      <span class="kbd">[B, 1]</span> before passing it into the FCBlocks.
    </li>
    <li>
      <strong>Where conditioning enters:</strong> Following the diagram/pseudocode, the first time embedding
      modulates the <span class="kbd">unflatten</span> features, and the second time embedding modulates
      the <span class="kbd">up1</span> features.
    </li>
    <li>
      <strong>Broadcasting:</strong> The FCBlock outputs are broadcast across spatial dimensions to match the
      feature map shapes during multiplication.
    </li>
  </ul>

  <pre class="codeblock"><code># t is an integer timestep sampled in [0, 999]
t_norm = (t / 999.0).view(B, 1)

t1 = fc1_t(t_norm)
t2 = fc2_t(t_norm)

# Replace with time-modulated versions
unflatten = unflatten * t1
up1 = up1 * t2</code></pre>

  <p>
    This modification allows the UNet to learn a <em>time-dependent</em> vector field for flow matching,
    instead of a single static denoising behavior.
  </p>
</div>
    </section>

    <!-- ====================== B.2.2 Training ====================== -->
    <section class="section" id="pb22-train">
      <h2>Part B.2.2 — Training the Time-Conditioned UNet</h2>
      <p>
        I train the time-conditioned UNet to predict the flow
        <span class="kbd">u_t = x - z</span> under linear interpolation between a clean
        image <span class="kbd">x</span> and a noise sample <span class="kbd">z</span>.
        I use the recommended hidden size <span class="kbd">D = 64</span>,
        batch size <span class="kbd">64</span>, Adam with initial learning rate
        <span class="kbd">1e-2</span>, and an exponential LR decay scheduler.
      </p>

      <div class="callout">
        <strong>Training setup</strong>
        <ul>
          <li>Model: time-conditioned UNet, <span class="kbd">D = 64</span></li>
          <li>Batch size: <span class="kbd">64</span></li>
          <li>Optimizer: Adam, <span class="kbd">lr = 1e-2</span></li>
          <li>Scheduler: ExponentialLR, <span class="kbd">gamma = 0.5</span></li>
        </ul>
      </div>

      <figure class="figure">
        <img src="p5/timecond_loss_curve.png" alt="Time-conditioned UNet loss curve"/>
        <figcaption class="figcap">
          Training loss curve for the time-conditioned flow matching UNet.
        </figcaption>
      </figure>
    </section>

    <!-- ====================== B.2.3 Sampling ====================== -->
    <section class="section" id="pb23-sample">
      <h2>Part B.2.3 — Sampling from the Time-Conditioned UNet</h2>
      <p>
        I generate samples by starting from pure Gaussian noise and iteratively updating the image
        using the predicted flow across timesteps. I show sampling quality as training progresses.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Sampling results after <span class="kbd">1</span>, <span class="kbd">5</span>, and <span class="kbd">10</span> epochs.</li>
        </ul>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5/samples_epoch1.png" alt="Flow matching samples after epoch 1"/>
          <figcaption class="figcap">Epoch <span class="kbd">1</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/samples_epoch5.png" alt="Flow matching samples after epoch 5"/>
          <figcaption class="figcap">Epoch <span class="kbd">5</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/samples_epoch10.png" alt="Flow matching samples after epoch 10"/>
          <figcaption class="figcap">Epoch <span class="kbd">10</span>.</figcaption>
        </figure>
      </div>
    </section>

    <!-- ====================== B.2.4+ Class Conditioning ====================== -->
    <section class="section" id="pb24-class">
      <h2>Part B.2.4 — Adding Class Conditioning</h2>
      <p>
        To improve sample quality and control, I condition the UNet on the digit class
        using one-hot vectors. I apply classifier-free dropout by zeroing the class conditioning
        10% of the time.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Training loss curve for the class-conditioned UNet.</li>
          <li>CFG sampling results after <span class="kbd">1</span>, <span class="kbd">5</span>, and <span class="kbd">10</span> epochs.</li>
          <li>Generate 4 instances of each digit class.</li>
        </ul>
      </div>

      <figure class="figure">
        <img src="p5/classcond_loss_curve.png" alt="Class-conditioned UNet loss curve"/>
        <figcaption class="figcap">
          Training loss curve for class-conditioned flow matching UNet.
        </figcaption>
      </figure>

  <!-- ====================== WITH SCHEDULER ====================== -->
  <h2>With exponential learning rate scheduler</h2>
  <p>
    I follow the recommended setup for class-conditioned flow matching with an exponential LR decay.
    The scheduler helps stabilize later-stage refinement while keeping early training fast.
  </p>

  <h3>Training loss (with scheduler)</h3>
  <div class="gallery">
    <figure class="figure">
      <img src="p5/loss_with_scheduler.png" alt="Class-conditioned training loss with scheduler"/>
      <figcaption class="figcap">
        Training loss curve for the class-conditioned, time-conditioned UNet
        using an exponential LR scheduler.
      </figcaption>
    </figure>
  </div>

  <h3>Sampling results (with scheduler)</h3>
  <p class="proj-sub" style="margin-top:-6px;">
    Each panel contains 4 samples per digit (0–9) using CFG with <span class="kbd">scale = 5.0</span>.
  </p>
  <div class="gallery">
    <figure class="figure">
      <img src="p5/samples_with_scheduler_ep1.png" alt="Class-conditioned samples with scheduler epoch 1"/>
      <figcaption class="figcap">Epoch <span class="kbd">1</span> (with scheduler).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/samples_with_scheduler_ep5.png" alt="Class-conditioned samples with scheduler epoch 5"/>
      <figcaption class="figcap">Epoch <span class="kbd">5</span> (with scheduler).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/samples_with_scheduler_ep10.png" alt="Class-conditioned samples with scheduler epoch 10"/>
      <figcaption class="figcap">Epoch <span class="kbd">10</span> (with scheduler).</figcaption>
    </figure>
  </div>

  <!-- ====================== WITHOUT SCHEDULER ====================== -->
  <h2>Without scheduler (constant learning rate)</h2>
  <p>
    I remove the exponential learning rate scheduler and train with a constant learning rate,
    then compare convergence and sample quality to the scheduled setup.
  </p>

  <div class="answer">
    <div class="label">Compensation for the loss of the scheduler</div>
    <ul>
      <li><strong>Strategy:</strong> I used a robust constant learning rate of <span class="kbd">1e-3</span>.</li>
      <li>
        <strong>Why this works:</strong> A scheduler typically starts high (e.g., <span class="kbd">1e-2</span>)
        to explore quickly and then decays low (e.g., <span class="kbd">1e-4</span>) to fine-tune.
        Without decay, <span class="kbd">1e-2</span> can cause late-stage oscillation,
        while <span class="kbd">1e-4</span> slows early learning too much.
      </li>
      <li>
        <strong>The “Goldilocks” value:</strong> <span class="kbd">1e-3</span> is a reliable Adam default—
        large enough for fast early structure learning and small enough for stable later refinement.
      </li>
    </ul>
  </div>

  <h3>Training loss (no scheduler)</h3>
  <div class="gallery">
    <figure class="figure">
      <img src="p5/loss_no_scheduler.png" alt="Class-conditioned training loss without scheduler"/>
      <figcaption class="figcap">
        Training loss curve for the class-conditioned, time-conditioned UNet
        using a constant learning rate (no scheduler).
      </figcaption>
    </figure>
  </div>

  <h3>Sampling results (no scheduler)</h3>
  <p class="proj-sub" style="margin-top:-6px;">
    Each panel contains 4 samples per digit (0–9) using CFG with <span class="kbd">scale = 5.0</span>.
  </p>
  <div class="gallery">
    <figure class="figure">
      <img src="p5/samples_no_scheduler_ep1.png" alt="Class-conditioned samples without scheduler epoch 1"/>
      <figcaption class="figcap">Epoch <span class="kbd">1</span> (no scheduler).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/samples_no_scheduler_ep5.png" alt="Class-conditioned samples without scheduler epoch 5"/>
      <figcaption class="figcap">Epoch <span class="kbd">5</span> (no scheduler).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/samples_no_scheduler_ep10.png" alt="Class-conditioned samples without scheduler epoch 10"/>
      <figcaption class="figcap">Epoch <span class="kbd">10</span> (no scheduler).</figcaption>
    </figure>
  </div>

  <div class="answer">
    <div class="label">Analysis of visualizations</div>
    <ul>
      <li>
        <strong>Epoch 1:</strong> Clear early convergence. The model already captures the general strokes of each digit,
        with mild fuzziness in rounded shapes (e.g., <span class="kbd">0</span>, <span class="kbd">8</span>).
      </li>
      <li>
        <strong>Epoch 5:</strong> Digits sharpen significantly. CFG encourages alignment to class prototypes,
        reducing early-stage artifacts.
      </li>
      <li>
        <strong>Epoch 10:</strong> Samples look highly realistic with preserved intra-class variation
        (e.g., multiple plausible writing styles).
      </li>
    </ul>
  </div>

  <div class="answer">
    <div class="label">Loss curve analysis</div>
    <ul>
      <li><strong>Rapid descent:</strong> A steep early drop indicates the model quickly learns the flow field.</li>
      <li>
        <strong>Stable floor:</strong> The curve flattens without large spikes
        (suggesting the constant LR is not too high) and without overly slow drift
        (suggesting it is not too low).
      </li>
    </ul>
  </div>
    </section>


  </main>

  <footer class="container footer">
    <div>Project 5 · CS180/280A</div>
  </footer>
</body>
</html>
