<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Project 5A — Diffusion Models, Inpainting & Optical Illusions</title>
  <meta name="theme-color" content="#0f1226"/>
  <link rel="stylesheet" href="assets/style.css?v=2"/>
</head>
<body>
  <!-- Top breadcrumb -->
  <nav class="topbar">
    <div class="container" style="padding:10px 24px">
      <div class="breadcrumb">
        <a href="index.html">← Back to Home</a>
        <span>·</span>
        <span>Project 5A — Diffusion Models</span>
      </div>
    </div>
  </nav>

  <main class="container">

    <!-- ====================== OVERVIEW ====================== -->
    <section class="section" id="overview">
      <h1 class="proj-title">Project 5A — Diffusion Models, Sampling, Inpainting & Illusions</h1>
      <p class="proj-sub">
        In Part A, I explore diffusion models through the DeepFloyd IF pipeline. I implement the forward noising process,
        write custom sampling/denoising loops, and then adapt these loops for image-to-image editing, inpainting, and
        classic diffusion-based optical illusions (visual anagrams + hybrid images).
      </p>

      <div class="callout">
        <strong>High-level structure</strong>
        <ul>
          <li><strong>Part 0:</strong> Setup, prompt embeddings, and baseline text-to-image samples.</li>
          <li><strong>Part 1:</strong> Forward process + classical denoising, one-step denoising, iterative denoising,
            CFG sampling, SDEdit, inpainting, text-conditional edits.</li>
          <li><strong>Part 1.8–1.9:</strong> Visual Anagrams and Factorized Diffusion Hybrid Images.</li>
          <li><strong>(Optional):</strong> Extra explorations if applicable for the course version.</li>
        </ul>
      </div>

      <div class="callout">
        <strong>Jump to</strong>
        <div class="cta-row" style="margin-top:8px">
          <a class="btn secondary" href="#p0-setup">Part 0</a>
          <a class="btn secondary" href="#p11-forward">1.1</a>
          <a class="btn secondary" href="#p14-iter">1.4</a>
          <a class="btn secondary" href="#p16-cfg">1.6</a>
          <a class="btn secondary" href="#p17-sdedit">1.7</a>
          <a class="btn secondary" href="#p18-anagrams">1.8</a>
          <a class="btn secondary" href="#p19-hybrids">1.9</a>
        </div>
      </div>
    </section>

<!-- ====================== PART 0 ====================== -->
<section class="section" id="p0-setup">
  <h1 class="proj-title">Part 0 — Setup & Text-to-Image with DeepFloyd IF</h1>
  <p>
    I gained access to DeepFloyd IF via Hugging Face, generated prompt embeddings, and sampled Stage I/II
    text-to-image outputs. I explored a small prompt set that I later reused for visual anagrams and
    hybrid images.
  </p>

  <div class="callout">
    <strong>Prompt pool used for embeddings</strong>
    <ul>
      <li>a high quality picture</li>
      <li>an oil painting of a snowy mountain village</li>
      <li>a photo of the amalfi coast</li>
      <li>a photo of a man</li>
      <li>a photo of a hipster barista</li>
      <li>a photo of a dog</li>
      <li>an oil painting of people around a campfire</li>
      <li>an oil painting of an old man</li>
      <li>a lithograph of waterfalls</li>
      <li>a lithograph of a skull</li>
      <li>a man wearing a hat</li>
      <li>a high quality photo</li>
      <li>a rocket ship</li>
      <li>a pencil</li>
    </ul>
  </div>

  <div class="answer">
    <div class="label">Reproducibility</div>
    <ul>
      <li><strong>Seed used for Part A:</strong> <span class="kbd">180</span></li>
      <li>
        I keep this seed fixed for all subsequent sections to make comparisons across
        denoising schedules, CFG, and editing tasks consistent.
      </li>
    </ul>
  </div>

  <!-- ====================== num_inference_steps = 20 ====================== -->
  <h2>Baseline generations — num_inference_steps = 20</h2>
  <p>
    Below are three selected prompts at <span class="kbd">20</span> inference steps. For each prompt,
    I show both DeepFloyd stages side-by-side.
  </p>

  <!-- Set 1 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5/old_1_20.png" alt="Steps 20 set 1 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 1 · Stage I</strong> (64×64) — <em>an oil painting of an old man</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/old_2_20.png" alt="Steps 20 set 1 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 1 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <!-- Set 2 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5/mt_1_20.png" alt="Steps 20 set 2 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 2 · Stage I</strong> (64×64) — <em>a photo of a snowy mountain</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/mt_2_20.png" alt="Steps 20 set 2 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 2 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <!-- Set 3 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5/rabbit_1_20.png" alt="Steps 20 set 3 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 3 · Stage I</strong> (64×64) — <em>a photo of a rabbit</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/rabbit_2_20.png" alt="Steps 20 set 3 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 3 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <!-- ====================== num_inference_steps = 50 ====================== -->
  <h2>Baseline generations — num_inference_steps = 50</h2>
  <p>
    I repeat the same three prompts with <span class="kbd">50</span> inference steps to compare
    prompt alignment, sharpness, and global coherence against the 20-step results.
  </p>

  <!-- Set 1 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5/old_1_50.png" alt="Steps 50 set 1 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 1 · Stage I</strong> (64×64) — <em>an oil painting of an old man</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/old_2_50.png" alt="Steps 50 set 1 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 1 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <!-- Set 2 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5/mt_1_50.png" alt="Steps 50 set 2 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 2 · Stage I</strong> (64×64) — <em>a photo of a snowy mountain</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/mt_2_50.png" alt="Steps 50 set 2 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 2 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <!-- Set 3 -->
  <div class="gallery">
    <figure class="figure">
      <img src="p5/rabbit_1_50.png" alt="Steps 50 set 3 stage 1"/>
      <figcaption class="figcap">
        <strong>Set 3 · Stage I</strong> (64×64) — <em>a photo of a rabbit</em>
      </figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/rabbit_2_50.png" alt="Steps 50 set 3 stage 2"/>
      <figcaption class="figcap">
        <strong>Set 3 · Stage II</strong> — upsampled refinement of Stage I
      </figcaption>
    </figure>
  </div>

  <div class="answer">
    <div class="label">Quick reflection</div>
    <ul>
    <li><strong>20 steps:</strong> The Stage I results capture the rough semantics of each prompt, but the images can look softer and less stable in composition. Stage II helps, but some minor artifacts and weaker prompt-specific details can remain at this lower step count.</li>
    <li><strong>50 steps:</strong> The generations are noticeably more consistent and better aligned with the text. The “oil painting of an old man” shows clearer painterly structure, while the “snowy mountain” and “rabbit” prompts look more coherent with cleaner edges and more believable texture.</li>
    <li><strong>Stage I vs II:</strong> Stage I (64×64) establishes the coarse layout and color palette, while Stage II (256×256) acts as a refinement pass that adds high-frequency detail and improves the intended style (photo vs. oil painting), making the outputs feel substantially more complete.</li>
    </ul>
  </div>
</section>

    <!-- ====================== PART 1 ====================== -->
    <section class="section" id="p1-overview">
      <h1 class="proj-title">Part 1 — Sampling Loops & Diffusion-Based Editing</h1>
      <p>
        In this part I implement core diffusion mechanics: the forward process, one-step denoising,
        and accelerated iterative denoising using a strided schedule. I then extend the same loop with
        classifier-free guidance (CFG) and apply it to sampling, SDEdit-style image-to-image translation,
        and inpainting.
      </p>
    </section>

    <!-- ====================== 1.1 Forward Process ====================== -->
    <section class="section" id="p11-forward">
      <h2>Part 1.1 — Implementing the Forward Process</h2>
      <p>
        I implement <span class="kbd">forward(im, t)</span> using the cumulative noise schedule
        <span class="kbd">alphas_cumprod</span>. Starting from a clean Campanile image, I generate
        progressively noisier versions at increasing timesteps.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Forward function implementation.</li>
          <li>Noisy Campanile at <span class="kbd">t = 250, 500, 750</span>.</li>
        </ul>
      </div>

      <div class="figure">
<pre class="codeblock"><code>def forward(im, t):
    """
    Args:
      im : torch tensor of size (1, 3, 64, 64) representing the clean image
      t : integer timestep

    Returns:
      im_noisy : torch tensor of size (1, 3, 64, 64) representing the noisy image at timestep t
    """
    with torch.no_grad():
        alpha_t = alphas_cumprod[t].to(im.device)
        epsilon = torch.randn_like(im)
        im_noisy = torch.sqrt(alpha_t) * im + torch.sqrt(1 - alpha_t) * epsilon

    return im_noisy</code></pre>
  <figcaption class="figcap">
    Forward diffusion implementation used to generate noisy versions of the 64×64 Campanile at specific timesteps.
  </figcaption>
</div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5/campanile_clean.png" alt="Original Campanile"/>
          <figcaption class="figcap">Original 64×64 Campanile.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/campanile_t250.png" alt="Noisy Campanile at t=250"/>
          <figcaption class="figcap">Forward process at <span class="kbd">t=250</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/campanile_t500.png" alt="Noisy Campanile at t=500"/>
          <figcaption class="figcap">Forward process at <span class="kbd">t=500</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/campanile_t750.png" alt="Noisy Campanile at t=750"/>
          <figcaption class="figcap">Forward process at <span class="kbd">t=750</span>.</figcaption>
        </figure>
      </div>
    </section>

<!-- ====================== 1.2 Classical Denoising ====================== -->
<section class="section" id="p12-gaussian">
  <h2>Part 1.2 — Classical Denoising with Gaussian Blur</h2>
  <p>
    I denoise the noisy Campanile images from Part 1.1 using Gaussian blur. As expected, this classical baseline
    struggles to recover fine structure at higher noise levels compared to learned diffusion denoisers.
  </p>

  <div class="callout">
    <strong>Deliverables</strong>
    <ul>
      <li>For <span class="kbd">t = 250, 500, 750</span>, show the noisy image and my best Gaussian-denoised result side by side.</li>
    </ul>
  </div>

  <h3>t = 250</h3>
  <div class="gallery">
    <figure class="figure">
      <img src="p5/campanile_t250.png" alt="Noisy Campanile at t=250"/>
      <figcaption class="figcap">Noisy Campanile at <span class="kbd">t=250</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/gauss_t250.png" alt="Gaussian blur denoising at t=250"/>
      <figcaption class="figcap">Gaussian blur denoising at <span class="kbd">t=250</span>.</figcaption>
    </figure>
  </div>

  <h3>t = 500</h3>
  <div class="gallery">
    <figure class="figure">
      <img src="p5/campanile_t500.png" alt="Noisy Campanile at t=500"/>
      <figcaption class="figcap">Noisy Campanile at <span class="kbd">t=500</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/gauss_t500.png" alt="Gaussian blur denoising at t=500"/>
      <figcaption class="figcap">Gaussian blur denoising at <span class="kbd">t=500</span>.</figcaption>
    </figure>
  </div>

  <h3>t = 750</h3>
  <div class="gallery">
    <figure class="figure">
      <img src="p5/campanile_t750.png" alt="Noisy Campanile at t=750"/>
      <figcaption class="figcap">Noisy Campanile at <span class="kbd">t=750</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/gauss_t750.png" alt="Gaussian blur denoising at t=750"/>
      <figcaption class="figcap">Gaussian blur denoising at <span class="kbd">t=750</span>.</figcaption>
    </figure>
  </div>
</section>

<!-- ====================== 1.3 One-Step Denoising ====================== -->
<section class="section" id="p13-onestep">
  <h2>Part 1.3 — One-Step Denoising with the Pretrained UNet</h2>
  <p>
    I use the pretrained Stage I UNet to estimate the noise in each noisy Campanile image, then recover an
    estimate of the original image using the standard diffusion inversion formula. Unlike Gaussian blur,
    the UNet can project the input toward the natural-image manifold, though performance degrades as the
    noise level increases.
  </p>

  <div class="callout">
    <strong>Deliverables</strong>
    <ul>
      <li>For <span class="kbd">t = 250, 500, 750</span>:
        show the original image, the noisy image (from my forward process),
        and the one-step denoised estimate.</li>
    </ul>
  </div>

  <h3>t = 250</h3>
  <div class="gallery" style="grid-template-columns: repeat(3, minmax(0, 1fr));">
    <figure class="figure">
      <img src="p5/campanile_clean.png" alt="Original Campanile"/>
      <figcaption class="figcap">Original Campanile (64×64).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_t250.png" alt="Noisy Campanile at t=250"/>
      <figcaption class="figcap">Noisy Campanile at <span class="kbd">t=250</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/onestep_t250.png" alt="One-step denoised Campanile at t=250"/>
      <figcaption class="figcap">One-step denoised at <span class="kbd">t=250</span>.</figcaption>
    </figure>
  </div>

  <h3>t = 500</h3>
  <div class="gallery" style="grid-template-columns: repeat(3, minmax(0, 1fr));">
    <figure class="figure">
      <img src="p5/campanile_clean.png" alt="Original Campanile"/>
      <figcaption class="figcap">Original Campanile (64×64).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_t500.png" alt="Noisy Campanile at t=500"/>
      <figcaption class="figcap">Noisy Campanile at <span class="kbd">t=500</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/onestep_t500.png" alt="One-step denoised Campanile at t=500"/>
      <figcaption class="figcap">One-step denoised at <span class="kbd">t=500</span>.</figcaption>
    </figure>
  </div>

  <h3>t = 750</h3>
  <div class="gallery" style="grid-template-columns: repeat(3, minmax(0, 1fr));">
    <figure class="figure">
      <img src="p5/campanile_clean.png" alt="Original Campanile"/>
      <figcaption class="figcap">Original Campanile (64×64).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/campanile_t750.png" alt="Noisy Campanile at t=750"/>
      <figcaption class="figcap">Noisy Campanile at <span class="kbd">t=750</span>.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/onestep_t750.png" alt="One-step denoised Campanile at t=750"/>
      <figcaption class="figcap">One-step denoised at <span class="kbd">t=750</span>.</figcaption>
    </figure>
  </div>

  <div class="answer">
    <div class="label">Observations</div>
    <ul>
      <li>At <span class="kbd">t=250</span>, the UNet recovers recognizable structure and sharper edges than Gaussian blur.</li>
      <li>At <span class="kbd">t=500</span>, the estimate is still plausible but loses finer detail.</li>
      <li>At <span class="kbd">t=750</span>, one-step denoising struggles, motivating the iterative procedure in Part 1.4.</li>
    </ul>
  </div>
</section>


<!-- ====================== 1.4 Iterative Denoising ====================== -->
<section class="section" id="p14-iterative">
  <h2>Part 1.4 — Iterative Denoising with a Strided Schedule</h2>
  <p>
    I implement a strided DDPM-style reverse process using the pretrained DeepFloyd Stage I UNet.
    Starting from a moderately noisy Campanile, I iteratively predict the clean image estimate,
    step toward a less noisy timestep, and add the model-predicted variance. I display every 5th
    step of the denoising loop and compare the final result against one-step denoising and a
    Gaussian blur baseline.
  </p>

  <div class="callout">
    <strong>Deliverables</strong>
    <ul>
      <li>Create <span class="kbd">strided_timesteps</span> from 990 to 0 in steps of 30 and initialize the scheduler.</li>
      <li>Implement <span class="kbd">iterative_denoise</span>.</li>
      <li>Show the noisy Campanile every 5th loop of denoising.</li>
      <li>Compare final iterative result vs. one-step vs. Gaussian blur.</li>
    </ul>
  </div>

  <div class="figure">
<pre class="codeblock"><code>strided_timesteps = list(range(990, -1, -30))

stage_1.scheduler.set_timesteps(timesteps=strided_timesteps)

import numpy as np
def iterative_denoise(im_noisy, i_start, prompt_embeds, timesteps, display=True):
  image = im_noisy.clone()
  prompt_embeds = prompt_embeds.to(device).half()
  with torch.no_grad():
    for i in range(i_start, len(timesteps) - 1):
      t = timesteps[i]
      prev_t = timesteps[i+1]

      alpha_cumprod_t = alphas_cumprod[t].to(device)
      alpha_cumprod_t_prev = alphas_cumprod[prev_t].to(device)
      alpha_t = alpha_cumprod_t / alpha_cumprod_t_prev
      beta_t = 1 - alpha_t

      model_output = stage_1.unet(
          image,
          torch.tensor(t).to(device),
          encoder_hidden_states=prompt_embeds,
          return_dict=False
      )[0]

      noise_est, predicted_variance = torch.split(model_output, image.shape[1], dim=1)

      x0_est = (image - torch.sqrt(1 - alpha_cumprod_t) * noise_est) / torch.sqrt(alpha_cumprod_t)
      x0_est = torch.clamp(x0_est, -1, 1)

      coeff1 = (torch.sqrt(alpha_cumprod_t_prev) * beta_t) / (1 - alpha_cumprod_t)
      coeff2 = (torch.sqrt(alpha_t) * (1 - alpha_cumprod_t_prev)) / (1 - alpha_cumprod_t)
      pred_prev_image = coeff1 * x0_est + coeff2 * image
      if i < len(timesteps) - 1:
        pred_prev_image = add_variance(predicted_variance, torch.tensor(t).to(device), pred_prev_image)
      image = pred_prev_image
      if display and i % 5 == 0:
        print(f"Iteration {i}, t={t} ->; {prev_t}")
        vis_img = image[0].permute(1, 2, 0).cpu().float() / 2.0 + 0.5
        media.show_image(vis_img, height=128)

  clean = image.cpu().detach().numpy()
  clean = (clean / 2.0 + 0.5).squeeze().transpose(1, 2, 0)
  clean = np.clip(clean, 0, 1)

  return clean

prompt_embeds = prompt_embeds_dict["a high quality photo"]

i_start = 10
t_start_val = strided_timesteps[i_start]
im_noisy = forward(test_im, t).half().to(device)

print(f"Starting Denoising from Step {i_start} (t={t_start_val})")
media.show_image(im_noisy[0].permute(1,2,0).cpu().float() / 2.0 + 0.5, title="Initial Noisy Image")

clean_iterative = iterative_denoise(im_noisy,
                          i_start=i_start,
                          prompt_embeds=prompt_embeds,
                          timesteps=strided_timesteps)

with torch.no_grad():
    alpha_cumprod_start = alphas_cumprod[t_start_val].to(device)

    noise_est_onestep = stage_1.unet(
        im_noisy,
        torch.tensor(t_start_val).to(device),
        encoder_hidden_states=prompt_embeds.to(device).half(),
        return_dict=False
    )[0][:, :3]

    clean_one_step = (im_noisy - torch.sqrt(1 - alpha_cumprod_start) * noise_est_onestep) / torch.sqrt(alpha_cumprod_start)
    clean_one_step = torch.clamp(clean_one_step, -1, 1)

    clean_one_step_vis = clean_one_step[0].permute(1, 2, 0).cpu().float() / 2.0 + 0.5

clean_gaussian_vis = TF.gaussian_blur(im_noisy, kernel_size=5)[0].permute(1, 2, 0).cpu().float() / 2.0 + 0.5
print("Final Results Comparison:")
media.show_images(
    [clean_iterative, clean_one_step_vis, clean_gaussian_vis],
    titles=["Iteratively Denoised", "One-Step Estimate", "Gaussian Blur"],
    height=256
)</code></pre>
    <figcaption class="figcap">
      Strided iterative denoising implementation and comparison code used for the Campanile experiment.
    </figcaption>
  </div>

  <h3>Denoising progression (every 5th step)</h3>
  <p>
    The sequence below shows the Campanile becoming progressively less noisy as the loop advances.
  </p>

  <div class="gallery figure-center">
    <figure class="figure">
      <img src="p5/it_10.png" alt="Iteration 10 denoising result"/>
      <figcaption class="figcap">Iteration <span class="kbd">10</span> (t = 690 → 660).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/it_15.png" alt="Iteration 15 denoising result"/>
      <figcaption class="figcap">Iteration <span class="kbd">15</span> (t = 540 → 510).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/it_20.png" alt="Iteration 20 denoising result"/>
      <figcaption class="figcap">Iteration <span class="kbd">20</span> (t = 390 → 360).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/it_25.png" alt="Iteration 25 denoising result"/>
      <figcaption class="figcap">Iteration <span class="kbd">25</span> (t = 240 → 210).</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/it_30.png" alt="Iteration 30 denoising result"/>
      <figcaption class="figcap">Iteration <span class="kbd">30</span> (t = 90 → 60).</figcaption>
    </figure>
  </div>

  <h3>Final comparison</h3>
  <div class="gallery" style="grid-template-columns: repeat(3, minmax(0, 1fr));">
    <figure class="figure">
      <img src="p5/iter_final.png" alt="Final iteratively denoised Campanile"/>
      <figcaption class="figcap">Final iteratively denoised result.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/onestep_final.png" alt="Final one-step denoised Campanile"/>
      <figcaption class="figcap">One-step denoised estimate.</figcaption>
    </figure>
    <figure class="figure">
      <img src="p5/gaussian_final.png" alt="Final Gaussian blurred Campanile"/>
      <figcaption class="figcap">Gaussian blur baseline.</figcaption>
    </figure>
  </div>

  <div class="answer">
    <div class="label">Observations</div>
    <ul>
      <li>The iterative method steadily recovers structure and color as noise decreases across the strided schedule.</li>
      <li>Compared to one-step denoising, the multi-step approach produces a cleaner, more coherent reconstruction.</li>
      <li>Gaussian blur reduces high-frequency noise but cannot restore lost structure, especially at higher noise levels.</li>
    </ul>
  </div>
</section>

    <!-- ====================== 1.5 Sampling from Noise ====================== -->
    <section class="section" id="p15-sampling">
      <h2>Part 1.5 — Diffusion Model Sampling</h2>
      <p>
        By setting <span class="kbd">i_start = 0</span> and initializing with pure Gaussian noise, I use the
        iterative denoising loop to generate images from scratch using the weak prompt
        <span class="kbd">"a high quality photo"</span>.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Five sampled images.</li>
        </ul>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5/sample_1.png" alt="Sample 1"/>
          <figcaption class="figcap">Sample 1.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/sample_2.png" alt="Sample 2"/>
          <figcaption class="figcap">Sample 2.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/sample_3.png" alt="Sample 3"/>
          <figcaption class="figcap">Sample 3.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/sample_4.png" alt="Sample 4"/>
          <figcaption class="figcap">Sample 4.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5/sample_5.png" alt="Sample 5"/>
          <figcaption class="figcap">Sample 5.</figcaption>
        </figure>
      </div>
    </section>

    <!-- ====================== 1.6 CFG ====================== -->
    <section class="section" id="p16-cfg">
      <h2>Part 1.6 — Classifier-Free Guidance (CFG)</h2>
      <p>
        I implement <span class="kbd">iterative_denoise_cfg</span> by combining conditional and unconditional
        noise estimates. With a positive CFG scale, generation quality improves significantly at the cost of
        reduced diversity.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>CFG-enabled denoising loop implementation.</li>
          <li>Five samples of <span class="kbd">"a high quality photo"</span> using CFG.</li>
        </ul>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/sample_cfg_1.png" alt="Sample 1 with CFG"/>
          <figcaption class="figcap">Sample 1 (CFG).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sample_cfg_2.png" alt="Sample 2 with CFG"/>
          <figcaption class="figcap">Sample 2 (CFG).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sample_cfg_3.png" alt="Sample 3 with CFG"/>
          <figcaption class="figcap">Sample 3 (CFG).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sample_cfg_4.png" alt="Sample 4 with CFG"/>
          <figcaption class="figcap">Sample 4 (CFG).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sample_cfg_5.png" alt="Sample 5 with CFG"/>
          <figcaption class="figcap">Sample 5 (CFG).</figcaption>
        </figure>
      </div>
    </section>

    <!-- ====================== 1.7 SDEdit + Edits ====================== -->
    <section class="section" id="p17-sdedit">
      <h2>Part 1.7 — Image-to-Image Translation (SDEdit)</h2>
      <p>
        Using CFG from this point onward, I apply SDEdit by adding noise to a real image and then projecting it
        back to the natural image manifold. I vary the starting index
        <span class="kbd">i_start ∈ {1, 3, 5, 7, 10, 20}</span>
        to control edit strength.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Campanile edits across the required starting indices using the prompt <span class="kbd">"a high quality photo"</span>.</li>
          <li>Two additional personal images edited with the same procedure.</li>
        </ul>
      </div>

      <div class="figure figure-center">
        <img src="p5a/sdedit_campanile_grid.png" alt="SDEdit Campanile results grid"/>
        <figcaption class="figcap">
          SDEdit on the Campanile across increasing <span class="kbd">i_start</span> values (lower = stronger edits).
        </figcaption>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/sdedit_own1_grid.png" alt="SDEdit on personal image 1"/>
          <figcaption class="figcap">SDEdit on my image #1.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sdedit_own2_grid.png" alt="SDEdit on personal image 2"/>
          <figcaption class="figcap">SDEdit on my image #2.</figcaption>
        </figure>
      </div>

      <!-- 1.7.1 -->
      <div class="callout">
        <strong>1.7.1 — Non-photorealistic inputs</strong>
        <p style="margin:8px 0 0">
          I also apply the same procedure to one web image and two hand-drawn sketches to demonstrate how diffusion
          can project rough or stylized inputs onto a more realistic manifold.
        </p>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/sdedit_web_grid.png" alt="SDEdit on a web image"/>
          <figcaption class="figcap">Web image projected across noise levels.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sdedit_draw1_grid.png" alt="SDEdit on hand-drawn image 1"/>
          <figcaption class="figcap">Hand-drawn sketch #1 → diffusion edits.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/sdedit_draw2_grid.png" alt="SDEdit on hand-drawn image 2"/>
          <figcaption class="figcap">Hand-drawn sketch #2 → diffusion edits.</figcaption>
        </figure>
      </div>

      <!-- 1.7.2 -->
      <div class="callout">
        <strong>1.7.2 — Inpainting</strong>
        <p style="margin:8px 0 0">
          I implement inpainting by forcing pixels outside the mask to match the appropriately noised original
          image at each timestep.
        </p>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/inpaint_campanile_input.png" alt="Campanile input for inpainting"/>
          <figcaption class="figcap">Input Campanile.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/inpaint_campanile_mask.png" alt="Inpainting mask"/>
          <figcaption class="figcap">Binary edit mask.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/inpaint_campanile_result.png" alt="Campanile inpainted result"/>
          <figcaption class="figcap">Inpainted Campanile result.</figcaption>
        </figure>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/inpaint_own1_triplet.png" alt="Personal inpainting example 1"/>
          <figcaption class="figcap">My image #1 inpainting (input/mask/result).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/inpaint_own2_triplet.png" alt="Personal inpainting example 2"/>
          <figcaption class="figcap">My image #2 inpainting (input/mask/result).</figcaption>
        </figure>
      </div>

      <!-- 1.7.3 -->
      <div class="callout">
        <strong>1.7.3 — Text-conditional image-to-image</strong>
        <p style="margin:8px 0 0">
          Finally, I replace the generic prompt with my own prompts to steer edits toward specific semantic changes
          while still preserving underlying structure at lower noise levels.
        </p>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/text_edit_campanile_grid.png" alt="Text-guided edits on Campanile"/>
          <figcaption class="figcap">Campanile edits with a custom prompt across <span class="kbd">i_start</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/text_edit_own1_grid.png" alt="Text-guided edits on personal image 1"/>
          <figcaption class="figcap">My image #1 with text-guided edits.</figcaption>
        </figure>
      </div>
    </section>

    <!-- ====================== 1.8 Visual Anagrams ====================== -->
    <section class="section" id="p18-anagrams">
      <h2>Part 1.8 — Visual Anagrams (Flip Illusions)</h2>
      <p>
        I implement visual anagrams by denoising an image with two prompts simultaneously: one on the original image
        and one on a vertically flipped copy. Averaging the aligned noise estimates produces a single image that
        changes interpretation when flipped upside down.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Implemented <span class="kbd">visual_anagrams</span> function.</li>
          <li>Two flip illusions with paired prompts.</li>
        </ul>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/anagram1_upright.png" alt="Visual anagram 1 upright"/>
          <figcaption class="figcap">Illusion #1 (upright) — <em>Prompt A</em>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/anagram1_flipped.png" alt="Visual anagram 1 flipped"/>
          <figcaption class="figcap">Illusion #1 (flipped) — <em>Prompt B</em>.</figcaption>
        </figure>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/anagram2_upright.png" alt="Visual anagram 2 upright"/>
          <figcaption class="figcap">Illusion #2 (upright) — <em>Prompt A</em>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/anagram2_flipped.png" alt="Visual anagram 2 flipped"/>
          <figcaption class="figcap">Illusion #2 (flipped) — <em>Prompt B</em>.</figcaption>
        </figure>
      </div>
    </section>

    <!-- ====================== 1.9 Hybrid Images ====================== -->
    <section class="section" id="p19-hybrids">
      <h2>Part 1.9 — Hybrid Images with Factorized Diffusion</h2>
      <p>
        To create diffusion-based hybrid images, I combine low-frequency structure from one prompt’s noise estimate
        with high-frequency detail from another. This mirrors the classic hybrid image idea from Project 2,
        but performed in the noise-prediction space of a diffusion model.
      </p>

      <div class="callout">
        <strong>Deliverables</strong>
        <ul>
          <li>Implemented <span class="kbd">make_hybrids</span> function.</li>
          <li>Two hybrid results using paired prompts.</li>
        </ul>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p5a/hybrid1.png" alt="Hybrid image 1"/>
          <figcaption class="figcap">Hybrid #1 — <em>Prompt A (low-pass)</em> + <em>Prompt B (high-pass)</em>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p5a/hybrid2.png" alt="Hybrid image 2"/>
          <figcaption class="figcap">Hybrid #2 — <em>Prompt A (low-pass)</em> + <em>Prompt B (high-pass)</em>.</figcaption>
        </figure>
      </div>

      <div class="answer">
        <div class="label">Perception note</div>
        <p>
          Up close, the high-frequency concept tends to dominate; from farther away or at smaller display sizes,
          the low-frequency concept becomes more apparent.
        </p>
      </div>
    </section>

  </main>

  <footer class="container footer">
    <div>Project 5A · CS180/280A</div>
  </footer>
</body>
</html>
