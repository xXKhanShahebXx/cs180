<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Project 4 — Neural Radiance Field!</title>
  <meta name="theme-color" content="#0f1226"/>
  <link rel="stylesheet" href="assets/style.css?v=2"/>
</head>
<body>
  <!-- Top breadcrumb -->
  <nav class="topbar">
    <div class="container" style="padding:10px 24px">
      <div class="breadcrumb">
        <a href="index.html">← Back to Home</a>
        <span>·</span>
        <span>Project 4 — Neural Radiance Field!</span>
      </div>
    </div>
  </nav>

  <main class="container">
    <!-- Overview -->
    <section class="section" id="overview">
      <h1 class="proj-title">Neural Radiance Field!</h1>
      <p>
        In this project I implement a full Neural Radiance Field (NeRF) pipeline. I start by calibrating my own
        camera and capturing a 3D scan of a real object, then estimate per-view camera poses with ArUco tags, and
        finally undistort/crop the images into a NeRF-ready dataset. Later parts of the project train and evaluate
        NeRF models on both the provided Lego scene and my custom capture.
      </p>
      <div class="callout">
        <strong>High-level structure</strong>
        <ul>
          <li><strong>Part 0:</strong> Camera calibration, pose estimation, Viser visualization, and dataset packaging.</li>
          <li><strong>Part 1:</strong> Neural field fitting on 2D images (coordinate-based MLP + positional encoding).</li>
          <li><strong>Part 2:</strong> Training and visualizing NeRF on both the Lego dataset and my own object scan.</li>
        </ul>
      </div>
    </section>

    <!-- ====================== PART 0 ====================== -->
    <section class="section" id="p0-overview">
      <h1 class="proj-title">Part 0 — Camera Calibration, Pose Estimation &amp; Dataset Packaging</h1>
      <p>
        In this part I take my own photos with a phone camera, calibrate its intrinsics using ArUco tags,
        estimate camera poses for a 3D scan of an object, visualize the camera frustums in 3D with Viser,
        and finally undistort / crop / split the images into a NeRF-ready dataset
        <span class="kbd">my_data.npz</span>.
      </p>

      <div class="callout">
        <strong>Deliverables checklist</strong>
        <ul>
          <li><strong>0.1:</strong> Camera calibration script using ArUco tags + reported intrinsics/distortion &amp; reprojection error.</li>
          <li><strong>0.2:</strong> 30–50 photos of my chosen object next to a printed tag (fixed zoom/exposure, varying views).</li>
          <li><strong>0.3:</strong> Pose estimation script that exports <span class="kbd">transforms.json</span> and a Viser visualization of camera frustums (2 screenshots).</li>
          <li><strong>0.4:</strong> Dataset script that undistorts, crops, (optionally downsamples), splits into train/val/test, and saves <span class="kbd">my_data.npz</span> containing images, c2w matrices, and intrinsics.</li>
        </ul>
      </div>
    </section>

    <!-- ====================== 0.1: Calibration ====================== -->
    <section class="section" id="p01-calibration">
      <h2>Part 0.1 — Calibrating My Camera with ArUco Tags</h2>
      <p>
        I start by calibrating my phone camera using 30–50 images of printed 4×4 ArUco tags at a fixed zoom
        (no refocusing or digital zoom between shots). I vary the viewing angle and distance, similar to chessboard
        calibration: some views are close, some are oblique, and I try to keep the tags well lit and in focus.
      </p>

      <div class="callout">
        <strong>Implementation</strong>
        <ol>
          <li><strong>Tag model.</strong> I treat each square tag as lying on the z = 0 plane with known metric size
            <span class="kbd">tag_size</span> (default 0.02&nbsp;m). The 3D corners are
            <span class="kbd">[(0,0,0), (s,0,0), (s,s,0), (0,s,0)]</span>, matching OpenCV’s corner order
            (top-left → top-right → bottom-right → bottom-left).
          </li>
          <li><strong>Robust marker selection.</strong> For each calibration image, I detect ArUco markers with
            <span class="kbd">DICT_4X4_50</span> and pick the <em>largest</em> marker by contour area.
            This stabilizes calibration when multiple tags are visible or when some tags are partially occluded.
          </li>
          <li><strong>Collecting correspondences.</strong> For every usable view I append a (4×3) object-point array
            and its corresponding (4×2) image-point array. Images with no detected tags are safely skipped.</li>
          <li><strong>Calibration model.</strong> I run <span class="kbd">cv2.calibrateCamera</span> on all collected
            views, but strongly constrain distortion:
            <ul>
              <li>Only <span class="kbd">k1</span> is estimated.</li>
              <li>Tangential distortion is forced to zero (<span class="kbd">p1 = p2 = 0</span>).</li>
              <li>Higher-order radial terms (<span class="kbd">k2..k6</span>) are fixed to 0.</li>
            </ul>
            This avoids wild fisheye behaviour when fitting high-order terms from only 4 corners per view.
          </li>
          <li><strong>Error reporting.</strong> For each view I project the 3D tag corners back into the image,
            compute the per-view reprojection error, and report the mean over all views alongside the RMS error
            returned by OpenCV.
          </li>
          <li><strong>Saving calibration.</strong> Finally, I save both a human-readable YAML file and a quick-load NPZ:
            <span class="kbd">camera_calibration.yaml</span> and <span class="kbd">camera_calibration.npz</span>,
            which contain <span class="kbd">K</span>, <span class="kbd">dist</span>, image size, and error metrics.
          </li>
        </ol>
      </div>

      <div class="figure">
<pre class="codeblock"><code># Part 0.1 — run calibration
python part0.py \
  --task calibrate \
  --image_dir calibration \
  --pattern "*.jpg" \
  --tag_size 0.02 \
  --min_views 12 \
  --out_yaml camera_calibration.yaml \
  --out_npz camera_calibration.npz

# Core 3D tag model (world == tag coordinates)
def make_tag_object_points(tag_size: float) -&gt; np.ndarray:
    s = float(tag_size)
    return np.array([
        [0.0, 0.0, 0.0],  # top-left
        [s,   0.0, 0.0],  # top-right
        [s,    s,  0.0],  # bottom-right
        [0.0,  s,  0.0],  # bottom-left
    ], dtype=np.float32)</code></pre>
        <figcaption class="figcap">
          Calibration entry point and 3D tag geometry; each successful image contributes four 2D–3D correspondences.
        </figcaption>
      </div>

      <div class="answer">
        <div class="label">Calibration summary</div>
        <ul>
          <li><strong>Usable views:</strong> a subset of the 30–50 images where at least one tag was detected.</li>
          <li><strong>Errors:</strong> RMS reprojection error is low (&lt; 1&nbsp;px), and per-view errors cluster
            around this value, indicating a stable calibration.</li>
          <li><strong>Model:</strong> a simple pinhole camera with only <span class="kbd">k1</span> free is sufficient
            for my phone images; constraining higher-order terms prevents overfitting and extreme distortion.</li>
        </ul>
      </div>
    </section>

    <!-- ====================== 0.2: Capture ====================== -->
    <section class="section" id="p02-capture">
      <h2>Part 0.2 — Capturing a 3D Object Scan</h2>
      <p>
        With intrinsics fixed, I next capture a 3D scan of an object placed next to a printed ArUco tag.
        I keep the <strong>same camera and zoom level</strong> as in calibration to ensure that the intrinsics
        remain valid.
      </p>

      <div class="callout">
        <strong>Capture procedure</strong>
        <ul>
          <li>Print a single 4×4 ArUco tag (from the assignment’s generator) and leave a white border for reliable detection.</li>
          <li>Place the object and tag on a tabletop; treat the tag plane as the world coordinate frame (z = 0).</li>
          <li>Use the same phone and zoom setting as in Part 0.1; lock exposure/white balance if possible.</li>
          <li>Capture 30–50 images from varying azimuth/elevation angles, but roughly uniform distance so that
            the object fills ~50% of the frame.</li>
          <li>Avoid motion blur and strong lighting changes; I reshoot any obviously blurred or overexposed images.</li>
        </ul>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p4/object_scan_01.jpg" alt="Object scan example view 1"/>
          <figcaption class="figcap">Example object + tag view (front/side angle).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p4/object_scan_02.jpg" alt="Object scan example view 2"/>
          <figcaption class="figcap">Another view from a higher elevation.</figcaption>
        </figure>
      </div>
    </section>

    <!-- ====================== 0.3: Pose Estimation ====================== -->
    <section class="section" id="p03-pnp">
      <h2>Part 0.3 — Estimating Camera Poses with solvePnP</h2>
      <p>
        Using the calibrated intrinsics, I estimate the camera pose (camera-to-world transform) for every scan image.
        Each pose comes from a minimal PnP instance: the four ArUco corners in 3D (world) and their 2D projections
        in the image.
      </p>

      <div class="callout">
        <strong>Implementation</strong>
        <ol>
          <li><strong>Loading calibration.</strong> I load <span class="kbd">K</span>, distortion coefficients, and
            image size from <span class="kbd">camera_calibration.yaml</span> or the NPZ file.
          </li>
          <li><strong>Detection &amp; filtering.</strong> For each image in <span class="kbd">data/</span> I:
            <ul>
              <li>Verify that the resolution matches the calibration resolution.</li>
              <li>Detect ArUco markers and pick the largest marker in the frame.</li>
              <li>Skip the image if no tag is found (no crash on bad frames).</li>
            </ul>
          </li>
          <li><strong>PnP pose solve.</strong> With known 3D tag corners and detected 2D corners, I run
            <span class="kbd">cv2.solvePnP</span> (ITERATIVE). If it fails, I fall back to
            <span class="kbd">cv2.solvePnPRansac</span> for extra robustness.
          </li>
          <li><strong>Reprojection sanity check.</strong> For each solved pose I reproject the tag corners and compute
            the average L2 error in pixels; this is logged per frame as a quality indicator.</li>
          <li><strong>World frame convention.</strong> OpenCV returns <span class="kbd">X_cam = R X_world + t</span>
            (world-&gt;camera). I convert this to camera-to-world (c2w) with
            <span class="kbd">Rᵀ</span> and <span class="kbd">-Rᵀ t</span>, treating the tag as the world origin.
          </li>
          <li><strong>NeRF-style JSON.</strong> I collect all successful frames in a <span class="kbd">transforms.json</span>
            file that follows the typical NeRF format:
            <ul>
              <li>Top-level intrinsics: <span class="kbd">fl_x</span>, <span class="kbd">fl_y</span>, <span class="kbd">cx</span>, <span class="kbd">cy</span>, <span class="kbd">w</span>, <span class="kbd">h</span>, and radial/tangential distortion terms.</li>
              <li>A <span class="kbd">frames</span> list where each entry stores
                <span class="kbd">file_path</span>, <span class="kbd">transform_matrix</span> (flattened 4×4 c2w),
                and diagnostics like reprojection error and tag id.</li>
            </ul>
          </li>
          <li><strong>Optional undistortion for visualization.</strong> If an <span class="kbd">--undistort_dir</span> is
            given, I create undistorted/cropped debug copies using <span class="kbd">getOptimalNewCameraMatrix</span>;
            however, <em>transforms.json always points to the original raw images</em> so that the final dataset step
            is the only place where undistortion happens (avoids double-undistort).
          </li>
        </ol>
      </div>

      <div class="figure">
<pre class="codeblock"><code># Part 0.3 — pose estimation &amp; transforms.json
python part0.py \
  --task pose \
  --data_dir data \
  --data_pattern "*.jpg" \
  --calib_yaml camera_calibration.yaml \
  --calib_npz camera_calibration.npz \
  --tag_size 0.02 \
  --out_json transforms.json \
  --undistort_dir undistort_debug \
  --debug_draw \
  --viser</code></pre>
        <figcaption class="figcap">
          Command used to estimate per-image poses and launch a Viser server for 3D visualization.
        </figcaption>
      </div>

      <div class="gallery figure-center">
        <figure class="figure">
          <img src="p4/viser_cameras_view1.png" alt="Viser camera cloud view 1"/>
          <figcaption class="figcap">
            Camera frustums orbiting the object (view 1); each frustum is textured with its corresponding image.
          </figcaption>
        </figure>
        <figure class="figure">
          <img src="p4/viser_cameras_view2.png" alt="Viser camera cloud view 2"/>
          <figcaption class="figcap">
            Same camera cloud from a different viewpoint, illustrating the coverage in elevation.
          </figcaption>
        </figure>
      </div>

      <div class="answer">
        <div class="label">Pose estimation notes</div>
        <ul>
          <li>Frames where the tag is too small, occluded, or blurred are automatically skipped rather than breaking the pipeline.</li>
          <li>The resulting camera cloud roughly traces a ring around the object with modest vertical variation, a good configuration for NeRF training.</li>
          <li>By consistently treating the tag plane as world space and inverting OpenCV’s world-&gt;camera transform, the c2w matrices are directly usable in later parts.</li>
        </ul>
      </div>
    </section>

    <!-- ====================== 0.4: Dataset Packaging ====================== -->
    <section class="section" id="p04-dataset">
      <h2>Part 0.4 — Undistorting Images &amp; Creating a NeRF Dataset</h2>
      <p>
        The final step is to undistort/crop the images, optionally downsample them, and package everything into a single
        <span class="kbd">.npz</span> file with intrinsics and camera poses. This NPZ matches the format expected by
        the NeRF training code used later in the project.
      </p>

      <div class="callout">
        <strong>Dataset pipeline</strong>
        <ol>
          <li><strong>Load calibration &amp; transforms.</strong> I load <span class="kbd">K</span>,
            <span class="kbd">dist</span>, and the original image size from Part 0.1, then read all frames from
            <span class="kbd">transforms.json</span>. Each frame provides a file path and a 4×4 c2w matrix.
          </li>
          <li><strong>Optimal undistortion &amp; cropping.</strong> To remove distortion and avoid black borders, I:
            <ul>
              <li>Call <span class="kbd">cv2.getOptimalNewCameraMatrix(K, dist, ... , alpha=0)</span> to obtain a new camera matrix
                and a rectangular ROI of valid pixels.</li>
              <li>Undistort each image with <span class="kbd">cv2.undistort</span> using <span class="kbd">newK</span>, then crop to the ROI.</li>
              <li>Update the principal point by subtracting the crop offset: <span class="kbd">cx -= x_roi</span>,
                <span class="kbd">cy -= y_roi</span>.</li>
            </ul>
          </li>
          <li><strong>Optional resizing.</strong> If <span class="kbd">resize_scale ≠ 1.0</span>, I downsample the
            cropped images using <span class="kbd">cv2.INTER_AREA</span> and scale the intrinsics
            (<span class="kbd">fx, fy, cx, cy</span>) by the same factor.</li>
          <li><strong>RGB conversion &amp; dumping.</strong> The final training images are stored as uint8 RGB arrays
            in [0, 255]. Optionally, I also dump these processed images to a folder for sanity-check visualization.</li>
          <li><strong>Train/val/test split.</strong> I shuffle the frames with a fixed random seed and partition them
            into train/val/test according to user-specified ratios (default 80/10/10), ensuring non-empty splits when possible.</li>
          <li><strong>Saving NPZ.</strong> Finally, I save:
            <ul>
              <li><span class="kbd">images_train</span>, <span class="kbd">c2ws_train</span></li>
              <li><span class="kbd">images_val</span>, <span class="kbd">c2ws_val</span></li>
              <li><span class="kbd">c2ws_test</span> (test images use the same underlying image tensor as train/val)</li>
              <li>Intrinsics for the final resolution: <span class="kbd">focal</span> (alias for <span class="kbd">fx</span>),
                plus <span class="kbd">fx, fy, cx, cy</span>.</li>
            </ul>
          </li>
        </ol>
      </div>

      <div class="figure">
<pre class="codeblock"><code># Part 0.4 — build my_data.npz from transforms.json
python part0.py \
  --task dataset \
  --transforms transforms.json \
  --calib_yaml camera_calibration.yaml \
  --calib_npz camera_calibration.npz \
  --dataset_npz my_data.npz \
  --train_ratio 0.8 \
  --val_ratio 0.1 \
  --test_ratio 0.1 \
  --resize_scale 1.0 \
  --dataset_undistort_dir dataset_images</code></pre>
        <figcaption class="figcap">
          Single command that undistorts/crops all views, splits them into train/val/test, and writes
          <span class="kbd">my_data.npz</span>.
        </figcaption>
      </div>

      <div class="answer">
        <div class="label">Final dataset properties</div>
        <ul>
          <li><strong>Resolution:</strong> after cropping (and any downsampling), all images share the same <span class="kbd">H×W</span> shape and intrinsics.</li>
          <li><strong>Consistency:</strong> c2w matrices are exactly those exported in <span class="kbd">transforms.json</span>, so the NeRF sees a coherent camera rig.</li>
          <li><strong>Compatibility:</strong> the resulting NPZ mirrors the structure of the provided Lego dataset, allowing me to plug
            <span class="kbd">my_data.npz</span> directly into the NeRF training/validation code in Parts 1 and 2.</li>
        </ul>
      </div>
    </section>

    <!-- ===================================================== -->
    <!-- ====================== PART 1 ====================== -->
    <!-- ===================================================== -->

    <section class="section" id="p1-overview">
      <h1 class="proj-title">Part 1 — Neural Field for a 2D Image</h1>
      <p>
        Before jumping into full 3D NeRFs, I first implement a 2D <em>neural field</em> that fits a single RGB image.
        Instead of mapping 3D position + viewing direction to color and density, this network maps
        normalized 2D pixel coordinates <span class="kbd">(x, y)</span> to RGB values <span class="kbd">(r, g, b)</span>.
        The model is a coordinate-based MLP with sinusoidal positional encoding, trained with random-pixel minibatches.
      </p>

      <div class="callout">
        <strong>Deliverables checklist</strong>
        <ul>
          <li>Report model architecture (layers, width, positional encoding frequencies, learning rate).</li>
          <li>Show training progression on the provided test image and on one of my own images.</li>
          <li>Show a 2×2 grid of final reconstructions for 2 choices of width and 2 choices of max frequency <span class="kbd">L</span>.</li>
          <li>Plot the PSNR curve over training for one image.</li>
        </ul>
      </div>
    </section>

    <!-- ====================== 1.1 — Architecture & PE ====================== -->
    <section class="section" id="p11-arch">
      <h2>Part 1.1 — MLP Architecture &amp; Sinusoidal Positional Encoding</h2>
      <p>
        The neural field takes a normalized 2D coordinate <span class="kbd">p = (x, y) ∈ [0,1]²</span> and first applies
        NeRF-style sinusoidal positional encoding, then feeds the result into a fully-connected MLP that predicts an RGB
        color in <span class="kbd">[0,1]</span>.
      </p>

      <div class="callout">
        <strong>Model design</strong>
        <ul>
          <li><strong>Positional Encoding (PE).</strong> For each coordinate <span class="kbd">p</span>, I use
            <span class="kbd">L</span> frequency bands and implement:
            <div class="figure">
<pre class="codeblock"><code># gamma(p) = [p, sin(2^k pi p), cos(2^k pi p)] for k = 0..L-1
class PositionalEncoding(nn.Module):
    def __init__(self, in_dims=2, num_freqs=10, include_input=True):
        super().__init__()
        self.in_dims = in_dims
        self.num_freqs = num_freqs
        self.include_input = include_input
        self.register_buffer(
            "freq_bands",
            (2.0 ** torch.arange(num_freqs)).float() * math.pi
        )
        out_dim = (in_dims if include_input else 0) + 2 * in_dims * num_freqs
        self.out_dim = out_dim

    def forward(self, x):  # x: (N, 2) in [0,1]
        outs = []
        if self.include_input:
            outs.append(x)
        xb = x.unsqueeze(-1) * self.freq_bands  # (N, 2, L)
        outs.append(torch.sin(xb).reshape(x.shape[0], -1))
        outs.append(torch.cos(xb).reshape(x.shape[0], -1))
        return torch.cat(outs, dim=-1)</code></pre>
              <figcaption class="figcap">
                Sinusoidal positional encoding: for <span class="kbd">L=10</span>, the 2D coordinate is mapped to a
                42-D feature vector.
              </figcaption>
            </div>
          </li>
          <li><strong>Neural Field MLP.</strong> The neural field itself is a ReLU MLP:
            <span class="kbd">[Linear(width) + ReLU] × depth → Linear(3) → Sigmoid</span>.
            The final Sigmoid ensures outputs lie in <span class="kbd">[0,1]</span>.
            <div class="figure">
<pre class="codeblock"><code>class NeuralField2D(nn.Module):
    def __init__(self, in_dim, width=128, depth=4, out_dim=3):
        super().__init__()
        layers = []
        last = in_dim
        for _ in range(depth):
            layers += [nn.Linear(last, width), nn.ReLU(inplace=True)]
            last = width
        layers += [nn.Linear(last, out_dim), nn.Sigmoid()]
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        return self.net(x)</code></pre>
              <figcaption class="figcap">
                Base architecture used in Part 1: depth = 4, width in {64, 128, 256}, and
                <span class="kbd">L ∈ {4, 10}</span> for the PE sweep.
              </figcaption>
            </div>
          </li>
          <li><strong>Coordinate parameterization.</strong> I index each pixel by normalized coordinates:
            <span class="kbd">x = col / (W-1)</span>, <span class="kbd">y = row / (H-1)</span>, so the domain is
            always <span class="kbd">[0,1]²</span> regardless of resolution.
          </li>
        </ul>
      </div>

      <div class="answer">
        <div class="label">Reported base architecture</div>
        <ul>
          <li><strong>Depth:</strong> 4 hidden layers.</li>
          <li><strong>Width:</strong> 128 channels (sweep also uses 64 and 256).</li>
          <li><strong>PE frequencies:</strong> default <span class="kbd">L = 10</span> (sweep also at <span class="kbd">L = 4</span>).</li>
          <li><strong>Activation:</strong> ReLU, with a final Sigmoid on RGB.</li>
          <li><strong>Optimizer:</strong> Adam with learning rate <span class="kbd">1e-2</span>.</li>
          <li><strong>Batch size:</strong> 10,000 pixels per iteration.</li>
          <li><strong>Iterations:</strong> 1,200–1,500 depending on the experiment.</li>
        </ul>
      </div>
    </section>

    <!-- ====================== 1.2 — Dataloader & Training Loop ====================== -->
    <section class="section" id="p12-training">
      <h2>Part 1.2 — Random-Pixel Dataloader, Loss, and Training Loop</h2>
      <p>
        Training the neural field on every pixel at every step is wasteful and can exceed GPU memory for large images.
        Instead, I treat each pixel as a training sample and randomly subsample a large mini-batch of pixels each
        iteration.
      </p>

      <div class="callout">
        <strong>Dataloader + training setup</strong>
        <ol>
          <li><strong>Image preprocessing.</strong> I load the RGB image as a <span class="kbd">(H, W, 3)</span> uint8
            tensor, then normalize to <span class="kbd">[0,1]</span> and flatten to <span class="kbd">(H·W, 3)</span>
            for supervision:
            <span class="kbd">gt = img.float() / 255.0</span>.
          </li>
          <li><strong>Coordinate grid.</strong> I precompute all coordinates once:
            <div class="figure">
<pre class="codeblock"><code>def make_coords(H, W, device):
    ys = torch.linspace(0.0, 1.0, H, device=device)
    xs = torch.linspace(0.0, 1.0, W, device=device)
    ys, xs = torch.meshgrid(ys, xs, indexing="ij")
    return torch.stack([xs, ys], dim=-1).view(-1, 2)</code></pre>
              <figcaption class="figcap">
                Normalized coordinate grid used as input to the positional encoding.
              </figcaption>
            </div>
          </li>
          <li><strong>Random pixel batches.</strong> Each iteration samples
            <span class="kbd">batch_size = 10,000</span> random indices into the flattened coordinate/color arrays:
            <div class="figure">
<pre class="codeblock"><code>idx = torch.randint(0, H*W, (batch_size,), device=device)
coords = coords_all[idx]          # (B, 2)
target = gt[idx]                  # (B, 3)
pred = model(pe(coords))          # (B, 3)</code></pre>
              <figcaption class="figcap">
                Stochastic training over random pixels, equivalent to using a batch size of 10k.
              </figcaption>
            </div>
          </li>
          <li><strong>Loss &amp; optimizer.</strong> I use MSE loss between predicted and ground-truth colors, with Adam:
            <div class="figure">
<pre class="codeblock"><code>loss = F.mse_loss(pred, target)
optim = torch.optim.Adam(model.parameters(), lr=1e-2)
optim.zero_grad(set_to_none=True)
loss.backward()
optim.step()</code></pre>
              <figcaption class="figcap">
                Basic training step for the neural field.
              </figcaption>
            </div>
          </li>
          <li><strong>PSNR metric.</strong> Every <span class="kbd">eval_every</span> iterations, I render the full
            image and compute MSE and PSNR:
            <div class="figure">
<pre class="codeblock"><code>def psnr_from_mse(mse):
    return -10.0 * torch.log10(mse.clamp(min=1e-12))

with torch.no_grad():
    full_pred = render_full_image(model, pe, H, W, device)
    mse = F.mse_loss(full_pred.view(-1,3), gt)
    psnr = psnr_from_mse(mse).item()</code></pre>
              <figcaption class="figcap">
                PSNR (in dB) is used to track reconstruction quality during training.
              </figcaption>
            </div>
          </li>
        </ol>
      </div>

      <div class="figure">
<pre class="codeblock"><code># High-level training driver
img_uint8, result = run_one(
    image_path="wolf.jpg",   # test image
    max_side=256,
    L=10, width=128, depth=4,
    iters=1500,
    batch_size=10_000,
    lr=1e-2,
    eval_every=100,
    progress_snaps=[0, 50, 100, 300, 600, 1000, 1500],
    device=device,
)</code></pre>
        <figcaption class="figcap">
          Configuration used for the main experiment on the provided test image.
        </figcaption>
      </div>
    </section>

    <!-- ====================== 1.3 — Training Progression ====================== -->
    <section class="section" id="p13-progress">
      <h2>Part 1.3 — Training Progression on Two Images</h2>
      <p>
        I visualize how the neural field gradually fits the image by snapshotting the full reconstruction at
        different iterations. Below I show the evolution for both the provided test image and one of my own images.
      </p>

      <h3>Test image (provided)</h3>
      <p>
        The test image is a natural image (e.g., <span class="kbd">wolf.jpg</span>). I train with
        <span class="kbd">width = 128</span>, <span class="kbd">depth = 4</span>, and <span class="kbd">L = 10</span>.
      </p>

      <div class="gallery">
        <figure class="figure">
          <img src="p4/part1_test_gt.png" alt="Test image ground truth"/>
          <figcaption class="figcap">Ground-truth test image.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p4/part1_test_iter_000.png" alt="Test image reconstruction iter 0"/>
          <figcaption class="figcap">Iteration 0 — random initialization; blurry low-frequency guess.</figcaption>
        </figure>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p4/part1_test_iter_100.png" alt="Test image reconstruction iter 100"/>
          <figcaption class="figcap">Iteration 100 — coarse colors and large structures emerge.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p4/part1_test_iter_300.png" alt="Test image reconstruction iter 300"/>
          <figcaption class="figcap">Iteration 300 — edges and mid-frequency details start appearing.</figcaption>
        </figure>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p4/part1_test_iter_600.png" alt="Test image reconstruction iter 600"/>
          <figcaption class="figcap">Iteration 600 — textures sharpen; background becomes more consistent.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p4/part1_test_iter_1500.png" alt="Test image reconstruction iter 1500"/>
          <figcaption class="figcap">Iteration 1500 — final fit; visually close to the ground truth.</figcaption>
        </figure>
      </div>

      <div class="answer">
        <div class="label">Observations — test image</div>
        <ul>
          <li>The network first learns low-frequency color structure (global background and large shapes).</li>
          <li>As training continues, higher frequency details (edges, fur texture, small contrast variations) are filled in.</li>
          <li>By ~1,000–1,500 iterations the reconstruction is nearly indistinguishable from the source image.</li>
        </ul>
      </div>

      <h3>Your own image</h3>
      <p>
        I repeat the same experiment on one of my own images (e.g., <span class="kbd">linkedin.png</span>), using the
        same architecture and training hyperparameters. The behavior is similar, but the convergence speed and final
        PSNR depend on how complex the image is (fine details vs. large flat regions).
      </p>

      <div class="gallery">
        <figure class="figure">
          <img src="p4/part1_own_gt.png" alt="Own image ground truth"/>
          <figcaption class="figcap">Ground-truth personal image.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p4/part1_own_iter_000.png" alt="Own image reconstruction iter 0"/>
          <figcaption class="figcap">Iteration 0 — random initialization.</figcaption>
        </figure>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p4/part1_own_iter_100.png" alt="Own image reconstruction iter 100"/>
          <figcaption class="figcap">Iteration 100 — coarse layout and average colors are already visible.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p4/part1_own_iter_600.png" alt="Own image reconstruction iter 600"/>
          <figcaption class="figcap">Iteration 600 — boundaries and facial features sharpen.</figcaption>
        </figure>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p4/part1_own_iter_1500.png" alt="Own image reconstruction iter 1500"/>
          <figcaption class="figcap">Iteration 1500 — final reconstruction; high-frequency details closely match GT.</figcaption>
        </figure>
      </div>

      <div class="answer">
        <div class="label">Observations — own image</div>
        <ul>
          <li>Structured regions like backgrounds and clothing are captured quickly.</li>
          <li>Human features (eyes, mouth, hair edges) require more iterations and benefit heavily from higher PE frequencies.</li>
          <li>The model still tends to smooth extremely high-frequency textures, especially when width or <span class="kbd">L</span> are small.</li>
        </ul>
      </div>
    </section>

    <!-- ====================== 1.4 — Hyperparameter Sweep (2×2 Grid) ====================== -->
    <section class="section" id="p14-hparams">
      <h2>Part 1.4 — Effect of Width &amp; Positional Encoding Frequencies</h2>
      <p>
        To understand how capacity and frequency content affect the neural field, I sweep over:
      </p>
      <ul>
        <li><strong>Widths:</strong> <span class="kbd">64</span> and <span class="kbd">256</span></li>
        <li><strong>Max PE frequencies:</strong> <span class="kbd">L = 4</span> and <span class="kbd">L = 10</span></li>
      </ul>
      <p>
        For each combination, I train for 1,200 iterations and visualize the final reconstruction, yielding a 2×2 grid.
      </p>

      <div class="figure figure-center" style="max-width: 980px;">
        <img src="p4/part1_test_sweep_2x2.png" alt="2x2 grid of hyperparameter sweep results"/>
        <figcaption class="figcap">
          2×2 hyperparameter sweep on the test image:
          top row uses width = 64, bottom row width = 256; left column uses <span class="kbd">L = 4</span>,
          right column uses <span class="kbd">L = 10</span>.
        </figcaption>
      </div>

      <div class="answer">
        <div class="label">Hyperparameter effects</div>
        <ul>
          <li><strong>Low width, low L (64, 4):</strong> the network mainly captures global color gradients and big shapes.
            Fine textures and sharp edges are heavily smoothed; the result looks like a blurred painting.</li>
          <li><strong>Low width, high L (64, 10):</strong> adding higher frequencies lets the network represent sharper
            detail, but limited width can underfit in complex regions and introduce mild ringing artifacts.</li>
          <li><strong>High width, low L (256, 4):</strong> more channels increase capacity, but without high-frequency PE
            the model still struggles to capture very sharp corners and tiny details.</li>
          <li><strong>High width, high L (256, 10):</strong> best overall reconstruction: the network fits both global
            structure and fine texture, and visually matches the ground-truth image the most closely.</li>
        </ul>
      </div>

      <div class="figure">
<pre class="codeblock"><code># 2×2 sweep code (for the test image)
widths = [64, 256]
freqs  = [4, 10]
configs = [(w, L) for w in widths for L in freqs]

results = []
for (w, L) in configs:
    print("=== Sweep:", {"width": w, "L": L}, "===")
    _, r = run_one(
        "wolf.jpg",
        max_side=256,
        L=L, width=w, depth=4,
        iters=1200,
        batch_size=10_000,
        lr=1e-2,
        eval_every=200,
        progress_snaps=[1200],  # only save final
        device=device,
    )
    results.append(r)

grid_2x2(img1.numpy(), configs, results)</code></pre>
        <figcaption class="figcap">
          Driver code for the 2×2 width/frequency sweep.
        </figcaption>
      </div>
    </section>

    <!-- ====================== 1.5 — PSNR Curve ====================== -->
    <section class="section" id="p15-psnr">
      <h2>Part 1.5 — PSNR Curve Over Training</h2>
      <p>
        Finally, I plot the PSNR over training iterations for one image (here, the test image). Each evaluation point
        renders the full image with the current neural field and computes PSNR with respect to the ground truth.
      </p>

      <div class="figure figure-center" style="max-width: 640px;">
        <img src="p4/part1_psnr_curve_test.png" alt="PSNR curve vs iteration"/>
        <figcaption class="figcap">
          PSNR (dB) vs. iteration for the test image, using width = 128 and L = 10.
        </figcaption>
      </div>

      <div class="answer">
        <div class="label">PSNR behavior</div>
        <ul>
          <li>PSNR increases rapidly in the first few hundred iterations as low-frequency structure is learned.</li>
          <li>After ~600–800 iterations, gains become more gradual as the model refines medium- and high-frequency details.</li>
          <li>The curve flattens around 1,200–1,500 iterations, suggesting the model has reached a good fit given its capacity and PE frequencies.</li>
        </ul>
      </div>
    </section>

  </main>

  <footer class="container footer">
    <div>Project 4 · CS180/280A</div>
  </footer>
</body>
</html>
