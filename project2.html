<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <title>Project 2 — Fun with Filters & Frequencies</title>
  <meta name="theme-color" content="#0f1226"/>
  <link rel="stylesheet" href="assets/style.css?v=2"/>
</head>
<body>
  <!-- Top breadcrumb -->
  <nav class="topbar">
    <div class="container" style="padding:10px 24px">
      <div class="breadcrumb">
        <a href="index.html">← Back to Home</a>
        <span>·</span>
        <span>Project 2 — Fun with Filters & Frequencies</span>
      </div>
    </div>
  </nav>

  <main class="container">
    <!-- Intro -->
    <section class="section" id="overview">
      <h1 class="proj-title">Project 2 — Fun with Filters & Frequencies</h1>
    </section>

    <!-- Part 1 header -->
    <section class="section" id="part1">
      <h2>Part 1 — Filters and Edges</h2>
      <p>I implemented two spatial-domain 2D convolutions in NumPy and compared them against <span class="kbd">scipy.signal.convolve2d</span>. Then I computed partial derivatives, gradient magnitudes, and binarized edge maps; finally I constructed Gaussian kernels via <span class="kbd">cv2.getGaussianKernel</span>, formed DoG (Derivative-of-Gaussian) filters, and compared their results to finite differences.</p>
    </section>

    <!-- Part 1.1 -->
    <section class="section" id="part1-1">
      <h2>Part 1.1 — Convolution (NumPy-only) & Comparison with SciPy</h2>
      <p><strong>Implementations.</strong> I wrote two versions:</p>
      <ul>
        <li><span class="kbd">conv2d_4loop</span>: explicit loops over image <em>and</em> kernel indices.</li>
        <li><span class="kbd">conv2d_2loop</span>: loops over pixels only; the inner multiply+sum is vectorized.</li>
      </ul>

      <!-- Key code snippets (concise “idea” only) -->
<div class="gallery grid-code">
  <figure class="figure">
    <pre class="codeblock"><code># (Idea) Flip kernel → zero-pad → 4 nested loops
def conv2d_4loop(img, K, padding):
    k = np.flipud(np.fliplr(K))
    ph, pw = padding if isinstance(padding, tuple) else (padding, padding)
    x = np.pad(img, ((ph, ph), (pw, pw)), mode='constant', constant_values=0)
    H, W = img.shape; kh, kw = k.shape
    out = np.zeros((H, W))
    for i in range(H):
        for j in range(W):
            s = 0.0
            for u in range(kh):
                for v in range(kw):
                    s += x[i+u, j+v] * k[u, v]
            out[i, j] = s
    return out</code></pre>
    <figcaption class="figcap">Naive 4-loop: clearest but slowest (Python loops over pixels and kernel).</figcaption>
  </figure>

  <figure class="figure">
    <pre class="codeblock"><code># (Idea) Same setup → loop over pixels only → vectorized inner product
def conv2d_2loop(img, K, padding):
    k = np.flipud(np.fliplr(K))
    ph, pw = padding if isinstance(padding, tuple) else (padding, padding)
    x = np.pad(img, ((ph, ph), (pw, pw)), mode='constant', constant_values=0)
    H, W = img.shape; kh, kw = k.shape
    out = np.zeros((H, W))
    for i in range(H):
        for j in range(W):
            patch = x[i:i+kh, j:j+kw]
            out[i, j] = np.sum(patch * k)   
    return out</code></pre>
    <figcaption class="figcap">2-loop: much faster in practice; inner work is NumPy-vectorized.</figcaption>
  </figure>
</div>

      <div class="figure" style="margin-top:14px">
<pre><code># (Idea) Quick correctness check vs SciPy (same padding)
ref = convolve2d(img, K, mode='same', boundary='fill', fillvalue=0)
err = np.max(np.abs(conv2d_2loop(img, K, padding=(K.shape[0]//2, K.shape[1]//2)) - ref))
print(f"max|custom - scipy| = {err:.2e}")</code></pre>
        <div class="figcap">Use SciPy as a reference with identical boundary conditions.</div>
      </div>

      <div class="answer" style="margin-top:12px">
        <div class="label">Answer — Runtime & Boundary Handling</div>
        <p><strong>Runtime.</strong> All are <span class="kbd">O(H·W·kh·kw)</span>. In practice: <em>4-loop</em> (slowest) ≪ <em>2-loop</em> ≪ SciPy’s <span class="kbd">convolve2d</span> (fastest, optimized C/FFT). </p>
        <p><strong>Boundaries.</strong> I pad with zeros to keep output size the same (<span class="kbd">np.pad(..., mode='constant', constant_values=0)</span>). For fair comparison I call SciPy with <span class="kbd">mode='same', boundary='fill', fillvalue=0</span>. (Alternatives like <span class="kbd">reflect</span> can reduce edge artifacts.)</p>
      </div>
    </section>

    <!-- Part 1.2 -->
    <section class="section" id="part1-2">
      <h2>Part 1.2 — Partial Derivatives, Gradient Magnitude, and Binarized Edges</h2>
      <p>I use the finite-difference kernels <span class="kbd">Dx = [[1, 0, -1]]</span> and <span class="kbd">Dy = Dxᵀ</span>. I apply zero padding of width 1 along the axis of the derivative so output dimensions match the input. Gradient magnitude is <span class="kbd">√(gx² + gy²)</span>, normalized to [0,1].</p>

      <div class="gallery">
        <figure class="figure">
          <img src="p2/p12_gx_fd.png" alt="Partial derivative in x"/>
          <figcaption class="figcap">Partial derivative <span class="kbd">Iₓ</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p2/p12_gy_fd.png" alt="Partial derivative in y"/>
          <figcaption class="figcap">Partial derivative <span class="kbd">Iᵧ</span>.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p2/p12_gradmag_fd.png" alt="Gradient magnitude"/>
          <figcaption class="figcap">Gradient magnitude <span class="kbd">‖∇I‖</span> after normalization.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p2/p12_edges_fd.png" alt="Binarized edge map"/>
          <figcaption class="figcap">Binarized edges using a percentile-based global threshold.</figcaption>
        </figure>
      </div>

      <div class="answer">
        <div class="label">Noise vs. edge completeness — threshold choice</div>
        <p>I set the threshold automatically from the gradient-magnitude distribution using <span class="kbd">percentile=90</span>. This retains salient boundaries while suppressing weaker texture/noise. Larger percentiles simplify edges; smaller percentiles recover faint edges but admit noise.</p>
      </div>
    </section>

    <!-- Part 1.3 -->
    <section class="section" id="part1-3">
      <h2>Part 1.3 — Gaussian Smoothing & DoG (Derivative-of-Gaussian)</h2>
      <p>I construct Gaussian kernels with <span class="kbd">cv2.getGaussianKernel</span> conceptually; in code I generate a 2D Gaussian via a separable 1D vector (<span class="kbd">gaussian_kernel2d</span>). Two equivalent routes to gradients:</p>
      <ol>
        <li><em>Smooth then differentiate:</em> blur with <span class="kbd">G(σ)</span>, then convolve with <span class="kbd">Dx</span>/<span class="kbd">Dy</span>.</li>
        <li><em>Single pass DoG:</em> pre-convolve <span class="kbd">G</span> with <span class="kbd">Dx/Dy</span> to form DoG kernels; convolve image once per axis. I verify they match (up to boundary/float eps).</li>
      </ol>

      <div class="gallery">
        <figure class="figure">
          <img src="p13_gx_smooth.png" alt="Smoothed x-derivative"/>
          <figcaption class="figcap">Smoothed x-derivative (route 1).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p13_gy_smooth.png" alt="Smoothed y-derivative"/>
          <figcaption class="figcap">Smoothed y-derivative (route 1).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p13_gradmag_smooth.png" alt="Grad magnitude after smoothing"/>
          <figcaption class="figcap">Gradient magnitude after smoothing.</figcaption>
        </figure>
        <figure class="figure">
          <img src="p13_edges_smooth.png" alt="Edges after smoothing"/>
          <figcaption class="figcap">Binarized edges (same percentile for parity).</figcaption>
        </figure>
      </div>

      <div class="gallery">
        <figure class="figure">
          <img src="p13_gx_dog.png" alt="DoG x-derivative"/>
          <figcaption class="figcap">DoG x-derivative (route 2).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p13_gy_dog.png" alt="DoG y-derivative"/>
          <figcaption class="figcap">DoG y-derivative (route 2).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p13_gradmag_dog.png" alt="DoG grad magnitude"/>
          <figcaption class="figcap">DoG gradient magnitude (normalized).</figcaption>
        </figure>
        <figure class="figure">
          <img src="p13_edges_dog.png" alt="DoG edges"/>
          <figcaption class="figcap">Binarized edges from DoG magnitude.</figcaption>
        </figure>
      </div>
    </section>

    <!-- Repro notes -->
    <section class="section" id="repro">
      <h2>Reproducibility Notes</h2>
      <ul>
        <li>Images saved by the script are referenced directly (e.g., <span class="kbd">p12_gx_fd.png</span>).</li>
        <li>Padding strategy in my code: zero padding via <span class="kbd">np.pad(..., mode='constant', constant_values=0)</span> with widths set to keep “same-size” outputs.</li>
        <li>Normalization helper: <span class="kbd">normalize01</span> maps arrays to [0,1] for saving/visualization; <span class="kbd">to_uint8</span> writes PNGs.</li>
        <li>Thresholding: <span class="kbd">binarize(..., percentile=90)</span>; tweak percentile to taste.</li>
      </ul>
    </section>
  </main>

  <footer class="container footer">
    <div>Project 2 · CS180/280A</div>
  </footer>
</body>
</html>
